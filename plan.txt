-==IMPORTANT SECTION==-
AI Routing System Implementation Plan (JS - Short Names Version - 201% Detail)

Goal & Assumptions
    1.1. Goal: Build an effective, reliable, maintainable, and responsive personal AI coding assistant using JavaScript, with a primary focus on maximizing cost efficiency in LLM API utilization through intelligent resource allocation.
    1.2. Cost: [CRITICAL: PRIMARY OBJECTIVE] Achieve the maximum possible intelligent reduction of LLM API token costs (target 65-75%+ savings) through a dynamic, multi-level routing system and aggressive, multi-tiered caching mechanisms. The core driver is to minimize operational expenses while maintaining acceptable performance and response quality.
    1.3. Rout: Implement a system that intelligently and transparently routes user queries to the optimal AI model for each specific request. This routing decision will dynamically select between: 1) Local LLMs (running on user's machine, e.g., CodeLlama via Ollama), 2) Cheap Remote LLMs (value-priced cloud APIs), and 3) Premium Remote LLMs (high-performance, higher-cost APIs like GPT-4, Claude Opus). The routing logic will be based on a deep, multi-faceted analysis of:
        1.  Query Characteristics (qryInfo): Complexity (estimated token count, code vs. natural language), user intent (code generation, refactoring, debugging, explanation, question answering, auto-completion), and identified entities (symbols, file paths).
        2.  Project Context (ctxInfo): Rich context derived from static and potentially dynamic code analysis provided by the Hub Service, including project structure, symbol definitions, dependencies, and relevant code snippets.
        3.  Interaction History: Relevant parts of the conversation history (multi-turn context) to maintain coherence and context awareness.
        4.  User-Defined Strategies: User-configurable preferences for cost vs. quality (e.g., "prioritize cost", "balance cost and quality", "maximize quality regardless of cost").
    This necessitates a sophisticated, adaptive routing mechanism capable of learning and evolving over time based on performance data and user feedback.
    1.4. Resrc: Implement efficient and responsible management of all system resources, including:
        1.  LLM API Usage: Optimize token consumption, minimize API calls, respect rate limits and billing budgets of external LLM APIs.
        2.  Local Compute Resources: Efficiently utilize local CPU and GPU (if available) for running local LLMs, embedding generation, and code parsing, minimizing resource contention and impact on user's machine performance.
        3.  Memory Management: Optimize RAM usage for in-memory caches (L1 cache), Hub data representation, and telemetry aggregation, preventing memory leaks and excessive memory footprint.
        4.  Persistent Storage: Efficiently manage persistent storage (SQLite database) for L2 cache, telemetry data, and backup files, minimizing disk I/O and storage overhead.
        5.  Network Bandwidth: Minimize network traffic, especially for repeated queries (via caching) and large context transfers.
    1.5. Savin:
        1.5.1. Scale Benchmark: Project cost savings against a benchmark consumption profile of approximately 2 billion tokens per month.
        1.5.2. Savings Percentage Target: Achieve a target cost reduction of at least 65-75% (and ideally exceeding this) compared to a hypothetical baseline of using only the most expensive premium LLM for all queries. Actual savings percentage will heavily depend on real-world usage patterns, the effectiveness of the routing logic, and cache hit rates.
        1.5.3. Monetary Savings Value: Realize potential monthly monetary savings exceeding $13,000 USD based on the benchmark token consumption and targeted percentage reduction.
        1.5.4. PoC Time Constraint: [CRITICAL: TIME CONSTRAINT LIMITS INITIAL SCOPE] A minimal Proof-of-Concept (PoC) validating the absolute core request path (input -> minimal call -> output) is targeted within an extremely ambitious 24-hour timeframe. It is crucial to understand that achieving the full scope of "Enhanced" features, robust testing, and production-readiness described in this plan will require a significantly longer development effort spanning weeks to months, not days. The 24-hour PoC serves solely as a validation of basic technical feasibility and project setup, not a functional MVP.
    1.6. Design Priorities:
        1.6.1. User Experience & Perceived Performance: Prioritize minimizing both actual and perceived latency. Employ aggressive caching strategies, response streaming (SSE), and fully asynchronous operations to ensure a fluid and responsive user interaction. Provide clear visual feedback (spinners, progress updates) to manage perceived wait times effectively. Responsiveness of the application thread (event loop) is critical.
        1.6.2. JS Code Quality & Maintainability: Maintain functional clarity despite naming constraints through rigorous, mandatory JSDoc (@param, @returns, @typedef for all non-trivial code), strict ESLint/Prettier enforcing coding standards, modular design (ES Modules preferred), and thorough testing. Adherence to Single Responsibility Principle (SRP) and writing short, focused functions is key. [Consider adopting TypeScript for enhanced maintainability post-PoC].
        1.6.3. Proven Technologies and Best Practices: Leverage established, well-documented, and actively maintained Node.js libraries and frameworks. Adhere to industry-standard best practices for asynchronous programming, error handling, security, and observability.
        1.6.4. Comprehensive Observability and Monitoring: Integrate robust observability from the outset. Implement detailed, structured logging (Pino with context/crids), detailed metrics collection (Prometheus format, counters, gauges, histograms), and tracing capabilities (via Correlation IDs) for deep system visibility, debugging, performance analysis, and cost verification.
        1.6.5. LLM Dev Process: [IMPORTANT META-STRATEGY] Embrace the use of LLMs as development assistants throughout the implementation process itself. Leverage LLMs for code generation (scaffolding, boilerplate, utility functions), automated test case generation, JSDoc documentation, and potentially for code refactoring suggestions. Implement the mandatory LLM self-logging process (3.2) to encourage self-reflection, identify areas for improvement, and document the LLM-assisted development workflow.
-==IMPORTANT SECTION==-

-==IMPORTANT SECTION==-
UI/Design Considerations (Conceptual Frontend UI - Not in 24h PoC Scope)
    This section outlines a conceptual vision for a separate, graphical frontend user interface (GUI) that would interact with the backend API built in this plan. Developing this GUI is explicitly outside the scope of the 24-hour Proof-of-Concept and the estimated development time. The API is designed to be UI-agnostic and can be consumed by various frontends.
    *   Aesthetic Vision for a Modern Developer UI:
        *   Overall Theme: Dark, professional, and visually appealing, minimizing eye strain during prolonged coding sessions.
        *   Color Palette: Dominated by dark shades of blue and deep navies (e.g., 
#1A202C, 
#2D3748) providing a low-fatigue background, complemented by vibrant, high-contrast accent colors (Cyan 
#0BC5EA, Purple 
#805AD5, Green 
#48BB78 for success indicators, code syntax highlighting, and interactive elements. Red 
#F56565 reserved for error states and critical alerts). Ensure WCAG AA or AAA compliant color contrast ratios for text and interactive elements for accessibility.
        *   Layout and Information Hierarchy: Clean, minimalist design with a focus on information clarity and efficient workflow. Utilize negative space effectively to reduce visual clutter. Implement a logical and intuitive layout, potentially using a sidebar navigation for core features, a main content area for code and AI interactions, and a status bar for system feedback.
        *   Visual Elements and Depth: Employ subtle gradients and soft, layered shadows (box-shadow) to add visual depth and separate UI elements. Experiment with a "glassmorphism" effect (transparency with background blur and thin, light borders) for modal windows, secondary panels, or overlays to create a sense of layering and visual hierarchy.
        *   Typography and Code Rendering: Choose highly readable, modern sans-serif fonts (e.g., Inter, Roboto) for UI text elements (labels, menus, buttons). Use a clear, fixed-width font (e.g., Source Code Pro, Fira Code, JetBrains Mono) optimized for code display, with configurable font sizes and syntax highlighting themes (potentially customizable dark themes aligning with the overall UI palette).
        *   User Interactions and Feedback: Design for fluid and responsive user interactions. Implement subtle, non-blocking animations (~200-300ms transitions) for state changes, menu expansions, and loading indicators. Provide immediate visual feedback for user actions (button presses, menu selections). Utilize progress spinners (ora-inspired) and response streaming (token-by-token display) to manage perceived latency and provide continuous feedback during AI processing.
    *   Backend API and CLI Support for UI:
        *   API Server (srv.js): The API is designed to be the data provider for a rich frontend UI. API endpoints will consistently return structured JSON data for all non-streaming responses, facilitating easy parsing and rendering of data in the UI. This includes structured data for: Hub information (symbol lists, file trees, symbol details), telemetry and statistics (query counts, cost breakdowns, performance metrics in chart-ready formats), code diffs (suitable for side-by-side code comparison views). [CRITICAL] The /v1/query endpoint is specifically designed to support Server-Sent Events (SSE), enabling efficient, real-time streaming of AI-generated text token-by-token to the frontend, allowing for dynamic and visually engaging text rendering (e.g., animated typing effects). Standardized JSON error responses allow the UI to display errors consistently and informatively.
        *   CLI Interface (cli.js): While primarily a command-line tool, the CLI is designed with a degree of visual clarity in mind. Utilize chalk library for color-coding output (syntax highlighting, status messages, error/warning indicators) to improve readability in the terminal. Employ ora spinners to provide basic progress feedback for long-running operations. Consider offering structured output options (e.g., --json flag) for commands that return data, enabling easier parsing and integration with external tools or potential terminal-based UIs (TUIs).
-==IMPORTANT SECTION==-

-==IMPORTANT SECTION==-
Proof-of-Concept (PoC) Definition - Ultra-Minimal Core Functionality (24-Hour Target)
    2.1. PoC Goal: Deliver a bare minimum, functional Proof-of-Concept (PoC) within the [CRITICAL] extremely tight 24-hour timeframe. The PoC's sole purpose is to validate the fundamental technical feasibility of the core query processing pathway and establish a basic project structure. It is NOT intended to be a feature-complete Minimum Viable Product (MVP) and will lack most of the "Enhanced" features described in the full system plan. The PoC is a rapid technical spike to assess core integration and identify immediate roadblocks, not a user-ready tool.
    2.2. PoC Scope - Functionality to be Implemented (Strict 24-Hour Limit):
        2.2.1. Minimal Setup (Phase 1 Subset): Focus on the absolute minimum setup required to get a basic Node.js application running and configured. This includes:
            *   Project initialization (npm init -y).
            *   Installation of only essential core dependencies: dotenv, pino, axios (or node-fetch), minimist, uuid.
            *   Creation of a basic directory structure (src/, config/).
            *   Configuration of a very basic logger (logr.js with console output only) and minimal configuration loading (cfg.js reading API key from .env - no Zod validation in PoC). Setup a single .env file with one API key.
        2.2.2. Core Logic - Minimal Stubs and Hardcoding: Implement stubbed versions of the core services, prioritizing speed of implementation over functionality or robustness:
            *   ApiS.js (Minimal API Client Stub): Implement one function that makes a direct, hardcoded call to a single, pre-selected remote LLM API endpoint (e.g., OpenAI's chat completions API). Hardcode API key directly in the code (for PoC simplicity, NOT for production). Implement minimal error handling: log a generic error message, return null or an empty string on failure. No retry logic, no streaming support. Reads API key from cfg.js.
            *   RoS.js (Trivial Routing Stub): Implement a function that completely ignores the user query and always returns a pre-determined, hardcoded model identifier (the single model used in ApiS). No query analysis, no routing rules, no model selection logic.
            *   CtxB.js (Trivial Context Builder Stub): Implement a function that always returns a fixed, empty string or a very short, hardcoded string as the context for every query. No interaction with the Hub, no context selection logic.
        2.2.3. Interface - Minimal Command-Line Interface (CLI):
            *   cli.js (Basic CLI Entry Point): Implement a barebones command-line interface using process.argv or minimist for argument parsing. Implement a single command: ask <prompt>. The ask command should:
                1.  Generate a unique Correlation ID (crid) using uuid.v4().
                2.  Call the stubbed RoS function to "select" the hardcoded model.
                3.  Call the stubbed CtxB function to get the trivial context.
                4.  Call the minimal ApiS function to send the user-provided prompt (and the trivial context) to the pre-configured LLM API.
                5.  Print the raw, unformatted text response received from the API directly to stdout.
                6.  Use logr.js to log the start and end of the ask command execution, including the generated crid and any errors encountered. On error, print a generic error message to stderr.
    2.2.4. [CRITICAL EXPLICIT EXCLUSIONS FROM 24h PoC]: To achieve the 24-hour target, the PoC scope must strictly exclude the following features and functionalities:
        *   [CRITICAL EXCLUSION] NO Hub Service implementation (no Acorn parsing, no file watching, no hub.json generation or loading).
        *   [CRITICAL EXCLUSION] NO Code Editor Service implementation (no AST-based editing, no diff generation, no backup/undo).
        *   [CRITICAL EXCLUSION] NO Caching mechanism of any kind (neither L1 RAM cache nor L2 persistent cache).
        *   [CRITICAL EXCLUSION] NO Complex Routing logic (no rules engine, no query analysis beyond minimal intent detection, no adaptive routing strategies).
        *   [CRITICAL EXCLUSION] NO API Server (srv.js with Express/Fastify) - CLI-only PoC.
        *   [CRITICAL EXCLUSION] NO Response Streaming (Server-Sent Events or async iterators).
        *   [CRITICAL EXCLUSION] NO Advanced Error Handling (no custom error classes, minimal try/catch only).
        *   [CRITICAL EXCLUSION] NO Automated Tests (Unit, Integration, or E2E) - manual testing only.
        *   [CRITICAL EXCLUSION] NO Observability or Monitoring infrastructure (no Prometheus metrics, no Grafana dashboards, basic console logging only).
        *   [CRITICAL EXCLUSION] NO Resilience or Auto-Recovery mechanisms (no circuit breakers, no rate limiting).
        *   [CRITICAL EXCLUSION] NO Security Hardening measures (beyond basic API key reading from .env).
    2.3. PoC Success Criteria (Achievable within 24 Hours):
        *   Executing the command node src/cli.js ask "any coding-related prompt" from the terminal should result in:
            1.  The application successfully initializing and running without crashing.
            2.  A network request being sent to the pre-configured remote LLM API endpoint.
            3.  A text-based response (even if it's just a placeholder or an error message from the API itself) being printed to the standard output (stdout) of the terminal.
            4.  At least one structured log message (in JSON format) being written to the console (via Pino), containing a unique crid and indicating the start or completion of the ask command execution.
-==IMPORTANT SECTION==-

-==IMPORTANT SECTION==-
**4. Risks & Mitigation (Full System View - Post-PoC)**

- **Hub Err**: Robust Acorn parsing (tolerant, error handling per file), async `chokidar` watcher (worker thread, debounce, incremental updates), extensive parser unit tests. (Priority: High)
- **Bad Ctx**: Multiple strategies (Full Hub, AST-based SmartContext, RAG), adaptive selection (`RoS`), tokenizer-based size optimization & pruning (`gpt-tokenizer`), explicit feedback loop (`TelS`) for tuning `rules.json`. (Priority: High)
- **Latency**: Strict async I/O, end-to-end streaming (SSE/async iterator), L1 (`node-cache`) + L2 (`better-sqlite3` WAL) caching, background processing (Hub updates, cleanup). (Priority: High)
- **Edit Err**: AST-based localization (`acorn-walk`), pre-edit node checks, mandatory interactive mode (`--diff`,`--confirm`), reliable timestamped backups (`.bak.YYYYMMDDHHmmss`), robust undo (`_xdUndo` reads backup map). (Priority: High)
- **Sec**: API keys via Env Vars ONLY, configurable regex redaction (`redact` func), audit logging (Pino+CRID), input sanitization/validation (Zod schemas, path normalization/validation), API security (Helmet, Rate Limit). (Priority: High)
- **JS Complex**: ES Modules, Aggressive SRP, Explicit Manual DI, **Mandatory JSDoc**, Strict ESLint/Prettier (Husky hooks), [Consider TS post-PoC]. (Priority: Medium-High)
- **Deps**: Use stable/maintained libs, lock file (`package-lock.json`), regular `npm update`/`audit` in CI. (Priority: Medium)

* LLM Implementation Log (3.2): LLM must maintain `./LLM_THINKING.txt`, logging structured thoughts (`FILES_EDITED`,`OBSERVATIONS`,`IMPROVEMENT_IDEAS`,`CONFIDENCE [1-5]`) after every 5 file saves. Purpose: Self-reflection, debugging aid, refinement identification.
-==IMPORTANT SECTION==-

-==PHASE 1==-
Phase 1: Preparation (JS Enhanced - Full System Foundation) (Est: 2-4 hours; Realistic: 1-2 days)
    *   [Goal]: Establish a robust project foundation with necessary tooling, dependencies, basic configuration, logging, and project structure. This phase lays the groundwork for the entire system, ensuring quality and consistency from the start. The 24h PoC implements only a minimal subset of this phase.
    4.1. JS Env & Project Setup
        4.1.1. Node.js Installation & Verification: Install the latest Long-Term Support (LTS) version of Node.js (e.g., 20.x or later). Verify the installation by running node -v and npm -v in the terminal. Ensure consistent Node.js version usage across development and deployment environments (using .nvmrc or similar).
        4.1.2. Project Initialization & Configuration: Initialize the Node.js project using npm init -y. Configure essential fields in package.json (e.g., name, version, description, author, license, repository, engines.node). Critically, set "type": "module" in package.json to enable ES Module syntax (import/export) throughout the project for better code organization and static analysis potential.
        4.1.3. Core Dependency Installation: Install all core runtime dependencies required for the full system using npm install <package-name>:
            *   HTTP Client: axios (robust, feature-rich) or node-fetch (lighter, closer to browser Fetch API).
            *   Logging: pino (high-performance structured JSON logger).
            *   Configuration: dotenv (load environment variables from .env files for local development), zod (schema declaration and validation for configuration and API data).
            *   Code Analysis: acorn (fast JavaScript parser), acorn-walk (AST traversal utility).
            *   File Watching: chokidar (reliable cross-platform file system watcher).
            *   Caching: node-cache (fast in-memory L1 cache), better-sqlite3 (high-performance synchronous SQLite driver for L2 cache and telemetry).
            *   API Server: fastify (high-performance web framework, preferred over Express).
            *   CLI Framework: commander or yargs (robust command-line argument parsing and help generation).
            *   Utilities: ora (terminal spinners for user feedback), diff (or diff-match-patch for generating code differences), glob or fast-glob (file pattern matching for Hub scanning), uuid (generating unique correlation IDs).
            *   Resilience: opossum (circuit breaker implementation).
            *   Observability: prom-client (Prometheus metrics exposition).
            *   Tokenization (Optional but recommended): gpt-tokenizer (or similar, for estimating token counts locally).
            *   Embeddings/RAG (Optional): transformers.js (if using local embedding models), potentially a vector database client library (chromadb, lancedb, vectors).
        4.1.4. Development Dependency Installation: Install all development-time dependencies using npm install --save-dev <package-name>:
            *   Process Management: nodemon (automatic server restart during development).
            *   Linting & Formatting: eslint, eslint-config-prettier (integrates ESLint with Prettier), eslint-plugin-node, eslint-plugin-promise (Node.js specific linting rules), prettier (automatic code formatter).
            *   Git Hooks: husky (manage Git hooks easily), lint-staged (run linters/formatters on staged files before commit).
            *   Testing: vitest (modern, fast test framework, preferred over Jest), @vitest/coverage-v8 (code coverage reporting).
            *   API Testing: supertest (HTTP assertion library for testing API endpoints).
            *   CLI Testing: execa (execute external commands, useful for testing CLI).
            *   Mocking: mock-fs (mock the file system for isolated tests, use with caution).
            *   Database Tools: sqlite3 (CLI tool for inspecting SQLite databases during development/testing).
            *   Logging Formatting (Dev): pino-pretty (human-readable log output during development).
        4.1.5. Directory Structure Definition: Establish a clear and scalable directory structure within the src/ directory to organize code by responsibility and feature:
            *   src/core/: Contains core domain logic and business rules (e.g., RoutingService, ContextBuilderService, CodeEditorService).
            *   src/features/: Contains modules related to specific application features, often bridging infrastructure and core logic (e.g., hub/ [HubService, parser logic], api/ [API route definitions, schemas, handlers], cli/ [CLI command definitions, handlers]).
            *   src/infrastructure/: Contains modules responsible for interacting with external systems or low-level concerns (e.g., ai/ [ApiS, model clients], cache/ [CchS, L1/L2 implementation], db/ [SQLite setup, migrations], logger/ [Pino setup]).
            *   src/shared/: Contains reusable utility functions, constants, custom error classes, and potentially shared type definitions (if not using TypeScript).
            *   src/config/: Contains configuration loading logic, default settings, and Zod validation schemas (index.js, schema.js, defaults.js).
            Create corresponding directories under tests/ (e.g., tests/unit/core, tests/integration/features/hub, tests/e2e/cli, tests/fixtures/). Use index.js files judiciously for exporting modules from directories (barrel files), being mindful of potential circular dependencies.
        4.1.6. NPM Scripts Configuration: Configure useful scripts in package.json's scripts section to streamline common development tasks:
            *   dev: nodemon src/main.js | pino-pretty (Run application with auto-restart and pretty-printed logs).
            *   start: NODE_ENV=production node src/main.js (Run application in production mode).
            *   lint: eslint src/ --ext .js (Run ESLint code analysis).
            *   lint:fix: eslint src/ --ext .js --fix (Run ESLint and automatically fix issues).
            *   fmt: prettier src/ --write (Run Prettier to format code).
            *   test: vitest run (Run all tests once).
            *   test:watch: vitest (Run tests in watch mode).
            *   test:cov: vitest run --coverage (Run tests and generate coverage report).
            *   cli: node src/cli.js (Helper to run CLI commands, e.g., npm run cli -- ask ...).
            *   api: node src/main.js --mode api (Start the API server specifically).
            *   build: (If using a build step like esbuild src/main.js --bundle --outfile=dist/main.js --platform=node --format=esm).
            *   db:init: (Script to create initial SQLite database files if they don't exist).
            *   db:migrate: (Script to run database migrations using node-sqlite-migrate or similar).
    4.2. API Configuration & Security Foundation
        4.2.1. Secure API Key Management: Implement logic in config/index.js to load API keys exclusively from environment variables (process.env.MODEL_X_API_KEY). Use dotenv.config() to load variables from a .env file for local development only. Implement strict runtime checks on application startup to ensure all required API keys (as defined in config/schema.js) are present in the environment. If any critical key is missing, throw a specific custom error (ConfigurationError) and prevent the application from starting (process.exit(1)). [CRITICAL SECURITY] Never commit .env files containing secrets to version control. Document required env vars in a .env.example.
        4.2.2. Initial API Availability Testing: Implement an asynchronous function async _xdChkTst(modelName) within the ApiS module (infrastructure/ai/ApiS.js). This function should make a lightweight, non-destructive test call (e.g., list models, get account info) to the specific model's API endpoint to verify both network connectivity and authentication credentials. Handle specific error conditions like timeouts, DNS errors, 401 Unauthorized, and 403 Forbidden distinctly and log them. On application startup (in main.js after ApiS is created), iterate through all configured models in cfg.modelCapabilities and call _xdChkTst for each. Log the success or failure status for each model. This initial check informs the RoS about which models are potentially available at startup and can populate initial runtime availability status.
        4.2.3. Data Redaction Setup: Define a configurable list of regular expression patterns in config/defaults.js aimed at identifying common sensitive data formats (e.g., typical API key structures like sk-..., common password patterns, email addresses, potentially specific internal identifiers or sensitive data formats used within the codebase). Implement a utility function function redact(inputText) in shared/utils.js that takes a string and applies these regex patterns to replace matches with a placeholder like [REDACTED] or *****. This function will be used later by CtxB (before sending context to AI) and potentially by logging interceptors (before logging potentially sensitive request/response bodies).
    4.3. System Architecture Principles (JS Modules & DI)
        4.3.1. ES Module Structure & Boundaries: Strictly use ES Module syntax (import/export) throughout the project (.js files with "type": "module"). Organize code into well-defined modules and place them in logical directories (src/core/, src/features/, src/infrastructure/, src/shared/). Enforce clear boundaries between modules and layers (e.g., core logic shouldn't directly interact with file system or external APIs; infrastructure handles external interactions and exposes a clean API to core/features). Each module should ideally have a single primary responsibility (Single Responsibility Principle - SRP). Avoid excessively large files; decompose logic into smaller, focused modules.
        4.3.2. Explicit Manual Dependency Injection (DI): Implement Dependency Injection by passing required dependencies (e.g., configuration object, logger instance, other service instances, database connections) as explicit arguments to module factory functions or class constructors. Example: function createRoutingService(config, logger, apiService, contextBuilderService, telemetryService) { /* ... */ return { routeQuery }; }. This promotes loose coupling, testability, and makes dependencies explicit. [CRITICAL] Avoid using global variables, service locators accessed globally, or implicit require() calls within modules for accessing dependencies. Document all injected dependencies clearly using JSDoc @param {ServiceType} dependencyName tags in function/constructor signatures.
    4.4. Hub Data Structure Definition (JSON Serialization & Zod Validation)
        4.4.1. Persistence Format & Compression: Define the primary persistence format for the analyzed Hub data as a compressed JSON file (hub.json.gz). Utilize Node.js's built-in zlib module (zlib.gzipSync, zlib.gunzipSync) for efficient compression/decompression of the JSON string during file write/read operations in HubS.js to minimize disk space usage and I/O time. Consider alternative binary formats like MessagePack (@msgpack/msgpack) as a potential future optimization if JSON parsing/serialization becomes a performance bottleneck for very large hubs, selectable via configuration.
        4.4.2. Zod Schema Definition (config/hubSchema.js): Define a detailed and strict Zod schema (hubSchema) that precisely validates the structure and data types of the Hub JSON. The schema should cover the top-level structure (e.g., a version identifier for the schema, an array of entries). Define a nested schema for each HubEntry representing information extracted from a code element: name (string, e.g., function name), path (string, relative file path), type (string enum, e.g., 'function', 'class', 'variable', 'import', 'export'), precise startPos and endPos (nested objects with line: number, col: number, offset: number from parser locations), jsdocSummary: z.string().optional() (extracted JSDoc content), dependencies: z.array(z.string()).optional() (list of identifiers of elements this entry depends on, e.g., imported module names), embedding: z.array(z.number()).optional() (vector embedding for RAG). Use short, consistent keys in the Zod schema (e.g., n, p, t, s, e, doc, deps, emb) if aiming for compact JSON storage, and map them to more descriptive names internally. Document each schema field with JSDoc comments explaining its purpose.
        4.4.3. Rigorous Data Validation: Implement strict validation using the hubSchema whenever Hub data is loaded from hub.json.gz (after decompression and JSON parsing) or whenever individual entries are updated/added (e.g., after parsing a changed file). Use hubSchema.parse(jsonData) or hubEntrySchema.parse(entryData). Catch any Zod validation errors (z.ZodError). Log detailed validation error messages (logger.error) including the specific validation issues and potentially the problematic data snippet or file path. Implement a strategy for handling validation failures (e.g., skip the invalid entry and continue, log a warning (logger.warn), or potentially fail the Hub loading process if data integrity is critical).
    4.5. Configuration Management (Layered & Validated)
        4.5.1. Configuration Module (config/index.js): Create a central module responsible for loading and merging configuration settings from various sources into a single, final configuration object. This module should export the final configuration object (made immutable with Object.freeze()) for injection into other services.
        4.5.2. Layered Configuration Sources: Implement a well-defined hierarchy for loading settings:
            1.  Load base default values from a config/defaults.js file (committed to Git). These provide sensible baseline settings.
            2.  Optionally load overrides from a config/config.json file (this file should be added to .gitignore and used for local environment-specific overrides not suitable for .env).
            3.  Load settings from environment variables (process.env). These are the highest priority and are typically used for secrets and production environment settings. Use dotenv.config() to load a .env file (gitignored) for local development environment variables.
            Implement robust merging logic to ensure environment variables override JSON and JSON overrides defaults. Log the effective source used for key configuration values (e.g., API keys source, log level source) in DEBUG mode during application startup for transparency.
        4.5.3. Zod Schema Validation (config/schema.js): Define a comprehensive Zod schema (configSchema) that validates the entire structure and data types of the final merged configuration object. This schema is the single source of truth for the application's expected configuration. Include detailed validation rules for: presence and format of API keys (z.string().min(1), potentially regex for specific key formats), structure and properties of modelCapabilities (nested Zod objects), format of routingRules (array of rule objects with specific condition/action structure), specific data types for numeric values (integers, positives), allowed values for enums (log levels, strategies). Add JSDoc comments explaining each configuration option's purpose. [CRITICAL] Immediately after merging configuration sources on application startup (in main.js or the config module), validate the merged object using configSchema.parse(mergedConfig). Catch any z.ZodError. If validation fails, log a detailed error message including the specific validation failures (error.issues) and the path within the config object causing the error. Prevent the application from starting by throwing a fatal ConfigurationError or calling process.exit(1) to ensure the application never runs with invalid configuration.
        4.5.4. Immutability and Caching: Ensure the final configuration object is made immutable using Object.freeze() after validation. This object should be instantiated only once on application startup and passed via Dependency Injection to all services that require it, ensuring consistency and preventing accidental modification at runtime.
    4.6. Logging System (Pino Structured & Contextual)
        4.6.1. Logger Initialization (infrastructure/logger.js): Create a factory function (createLogger(config)) that returns a configured singleton Pino logger instance. Configure essential Pino options: level set from config.logLevel, include base context (base: { pid: process.pid, hostname, env }), enable ISO 8601 timestamps (timestamp: pino.stdTimeFunctions.isoTime). Configure serializers for common object types (e.g., err: pino.stdSerializers.err, req: pino.stdSerializers.req, res: pino.stdSerializers.res) to log errors, requests, and responses consistently and effectively in structured format.
        4.6.2. Log Levels and Verbosity Control: Utilize standard log levels (trace, debug, info, warn, error, fatal) to control verbosity dynamically via the config.logLevel property (ultimately controllable via the LOG_LEVEL environment variable). trace level for highly detailed step-by-step execution tracing (e.g., function entry/exit, variable values in loops). debug for detailed development/debugging information. info for key operational events (request start/end, service initialized, significant state changes). warn for potential issues that don't stop execution. error for handled errors. fatal for unrecoverable errors leading to application exit.
        4.6.3. Structured JSON Output for Centralized Logging: Configure Pino to output logs in standard JSON format to stdout. This is the standard and most efficient method for modern containerized environments and cloud deployments, where logs are automatically collected by agents (Fluentd, Filebeat, Promtail) and forwarded to centralized log management systems (ELK Stack, Loki). For local development, use the pino-pretty tool piped to the standard output (npm run dev | pino-pretty) to format the JSON logs into a human-readable, colored output. If file logging is strictly required, use pino.destination for performance and manage rotation separately or use a library like pino-roll.
        4.6.4. Correlation IDs (CRID) for End-to-End Tracing: [CRITICAL FOR OBSERVABILITY & DEBUGGING] Implement a robust mechanism for generating and propagating a unique Correlation ID (crid) for every logical operation that flows through the system (e.g., each incoming API request, each CLI command execution, the start of each significant background task like a Hub rebuild).
            *   Generation: Use uuid.v4() to generate a crid at the entry point of the operation.
            *   Propagation: Pass the crid explicitly as a parameter through all function calls and service interactions related to that operation. Avoid relying on implicit context propagation mechanisms if possible, as explicit passing is clearer and more reliable in Node.js async code.
            *   Logging: Ensure that every single log message generated by the application includes its corresponding crid in the JSON log object. This is essential for tracing the flow of a single request/operation through the system in centralized logs.
            *   API Integration: Use the pino-http middleware with Fastify (Fastify({ logger: pinoHttp({ logger: deps.logr, genReqId: () => uuid.v4() }) })). Configure pino-http's genReqId option to use uuid.v4() for automatically generating a unique ID for each incoming HTTP request and injecting it into the request object (request.id), which pino-http then automatically includes in request-specific logs.
            *   CLI/Background Tasks: Manually generate the crid at the start of the handler/worker and pass it down the call stack to all service calls and log statements.
    4.7. Git Repository & CI/CD Pipeline Setup
        4.7.1. Comprehensive .gitignore: Create and maintain a thorough .gitignore file to prevent committing unnecessary or sensitive files. Exclude: node_modules/, build artifacts (dist/, coverage/), log files (*.log, npm-debug.log*), environment configuration files containing secrets (.env, .env.* but include .env.example), local database files (cache.db*, telemetry.db*), OS-specific files (.DS_Store, Thumbs.db), common IDE configuration directories (.vscode/, .idea/), and the LLM thinking log (LLM_THINKING.txt).
        4.7.2. Pre-commit Hooks (Husky + lint-staged): Configure husky to set up Git hooks (specifically the pre-commit hook). Configure lint-staged to run as the action for the pre-commit hook. Define lint-staged rules in package.json or a separate config file to automatically run code quality checks only on files that are currently staged for commit. Recommended actions: eslint --fix (automatically fix linting issues where possible) and prettier --write (automatically format code according to Prettier rules) on all staged .js files. This ensures that all committed code adheres to defined quality and formatting standards before it enters the repository.
        4.7.3. Continuous Integration (CI) Pipeline (GitHub Actions / GitLab CI): Define a CI pipeline configuration file (e.g., .github/workflows/ci.yml for GitHub Actions). The pipeline should trigger automatically on pushes to the main development branches (main, master) and on pull requests targeting these branches. Define sequential stages/jobs within the pipeline:
            1.  Checkout Code: Get the latest code from the repository.
            2.  Setup Node.js Environment: Set up the specified LTS Node.js version, ensuring consistency with the development environment (using .nvmrc or specified version).
            3.  Install Dependencies: Use npm ci for a clean and fast installation of dependencies based on the package-lock.json file, ensuring reproducible builds. Cache the node_modules directory between CI runs to speed up subsequent runs.
            4.  Code Quality Checks: Run static analysis tools like ESLint (npm run lint). Configure the job to fail if linting issues are found.
            5.  Run Automated Tests & Coverage: Execute the full suite of automated tests (npm run test --coverage). Ensure all tests pass. Integrate with a code coverage reporting service (e.g., Codecov, Coveralls) to track coverage trends and potentially enforce a minimum code coverage threshold (e.g., 80%) to ensure critical code paths are tested.
            6.  Security Vulnerability Scan: Run npm audit --audit-level=critical to check for known security vulnerabilities in project dependencies. Fail the job if critical vulnerabilities are found.
            7.  Build (Optional): If the project requires a build step (e.g., TypeScript compilation with tsc, JavaScript bundling/minification with esbuild or swc), execute the build script (npm run build).
            Configure the pipeline to fail the entire build if any of these stages fail. Provide clear logging and reporting for each stage's results.

-==PHASE 2==-
Phase 2: System Core - Implementation (JS Enhanced - Building the Engine) (Est: 8-12 hours; Realistic: Weeks)
    *   [Goal - Post-PoC]: Implement the core logical components that drive the AI routing and context building process. This phase focuses on functionality, robustness, testability, and adherence to the defined architecture and mitigation strategies. The 24h PoC only implements minimal stubs for these services.
    *   [Process Note]: Implement services iteratively based on dependencies (Cfg/Logr -> CchS/TelS -> HubS -> ApiS -> CtxB -> RoS -> EditS). Prioritize defining and implementing service interfaces (JSDoc @typedef) and basic class/factory structures before delving into complex logic. Write automated unit and integration tests concurrently with development (Test-Driven Development approach is highly recommended where practical) to ensure correctness and maintainability.
    5.1. Hub Service (features/hub/HubS.js)
        5.1.1. Asynchronous Initialization (async function createHubService(...) & async init() method): Implement a factory function that takes config, logger, telemetryService, and apiService (if embeddings via API) dependencies. The factory returns a Hub Service object with an async init() method. init() should:
            *   Use fast-glob to scan the project root directory (config.projectRoot) based on configured include/exclude patterns (e.g., config.hubScanPatterns, config.hubIgnorePatterns).
            *   Read contents of matched files asynchronously using fs.promises.readFile.
            *   Iterate through files, calling the parse function (5.1.2) for each.
            *   Aggregate the HubEntry arrays returned by parse into the in-memory Hub data structure (e.g., a Map<filePath, HubEntry[]> or a more complex structure optimized for lookups like symbol name -> list of entries).
            *   Handle file read errors (try/catch) gracefully (log error, skip file, continue).
            *   Persist the initial in-memory Hub state to config.hubFilePath (hub.json.gz) using zlib.gzipSync and fs.promises.writeFile.
            *   Provide progress indication during scanning and parsing via ora (in CLI mode) or by logging start/progress/completion events to telemetryService with the initialization crid.
        5.1.2. Acorn Parser (function parse(filePath, fileContent, config, logger, apiService)): Implement the core logic for parsing a single source file and extracting code structure information.
            *   Use acorn.parse(fileContent, { ecmaVersion: config.ecmaScriptVersion, locations: true, ranges: true, allowReturnOutsideFunction: true, allowImportExportEverywhere: true, tolerant: true }). Use tolerant: true with caution; robust error handling is preferred.
            *   Traverse the generated AST using acorn-walk.simple or acorn-walk.full.
            *   Identify relevant nodes based on node.type (e.g., FunctionDeclaration, VariableDeclarator, ClassDeclaration, MethodDefinition, ArrowFunctionExpression, ExportNamedDeclaration, ImportDeclaration, ObjectExpression for simple constants, CallExpression for function calls to identify dependencies).
            *   For each identified node, extract the required information to create a HubEntry object: name (identifier), filePath (relative path), type (map AST node type to consistent string enum), precise start/end position (using node.loc for line/col and node.start/node.end for offset), JSDoc summary (by analyzing leading comments associated with the node - may require separate comment parsing and association logic).
            *   Extract basic dependencies by analyzing function calls within bodies or module import paths (basic static analysis).
            *   Return an array of HubEntry objects for the file.
            *   Implement robust try/catch around the acorn.parse call and potentially around AST traversal logic for each file to catch syntax errors or parsing issues gracefully. Log errors (logger.warn) with file path and error details, but continue processing other files.
            *   [IMPORTANT - RAG Integration]: If RAG is enabled (config.features.rag.enabled), integrate embedding generation here. For relevant nodes (e.g., function bodies, class definitions, key constants), extract the source code snippet (fileContent.slice(node.start, node.end)). Call a separate function generateEmbedding(codeSnippet) within ApiS or a dedicated EmbeddingService. Pass the code snippet and relevant dependencies (config, ApiS). This function uses the configured embedding model (local via transformers.js or remote API via ApiS) to generate a vector embedding (number[]). Store the resulting vector in the HubEntry.embedding field. Handle embedding generation errors (log warning, proceed without embedding).
        5.1.3. File Watching (chokidar): Implement file system watching using chokidar.watch(config.projectRoot, { ignored: config.hubIgnorePatterns, ignoreInitial: true, awaitWriteFinish: true, ignorePermissionErrors: true }). To prevent blocking the main event loop, run the file watching and the subsequent file reading/parsing logic in a dedicated worker thread (infrastructure/hub/hubUpdateWkr.js) using Node.js worker_threads. The worker thread watches files, and upon detecting changes (add, change, unlink), it debounces the events (e.g., wait 500ms after the last event using lodash.debounce) to process batches of file changes. The worker then sends a message back to the main thread containing the list of changed file paths. The main thread's Hub Service manages a queue of files to be updated.
        5.1.4. Incremental Hub Update Logic (async function _xdUpdH(changedFiles, config, logger, telemetryService, apiService)): Implement the logic for processing file changes received from the watcher queue. For each file:
            *   If deleted (unlink event): Remove corresponding entries from the in-memory Hub Map by file path.
            *   If added/changed (add/change event): Read the file content (fs.promises.readFile), call the parse function (5.1.2), and update/replace the entries for that file in the in-memory Hub Map.
            *   After processing a batch of changes, rewrite the entire in-memory Hub state to config.hubFilePath (hub.json.gz) asynchronously (fs.promises.writeFile). [CRITICAL] Implement appropriate concurrency control (e.g., a simple lock or queue) to ensure that Hub data reads from other services (getHubSnapshot, findSymbol) are consistent during updates and that multiple update events don't cause race conditions with file writing. Log update process (start, files processed, duration) to TelS. If RAG enabled, ensure embeddings are updated or removed for affected code entries.
        5.1.5. Hub Service API: Define and implement asynchronous methods for other services (CtxB, EditS, API/CLI features) to query and access Hub data:
            *   async function getHubSnapshot(): Returns an immutable copy or snapshot of the current in-memory Hub data (to avoid external modification).
            *   async function findSymbol(nameQuery): Searches the in-memory Hub data for symbols whose names match the nameQuery (potentially supporting partial or fuzzy matching). Returns an array of matching HubEntry summaries.
            *   async function getSymbolDetails(symbolIdentifier: { filePath: string, offset: number }): Retrieves full details (including position, type, dependencies, potential code snippet or embedding) for a specific symbol identified by path/offset or a unique ID.
            *   async function getFileContent(filePath): Reads and returns the content of a specific file within the project. (Could be part of HubS or a separate FileService).
            *   async function getFileAst(filePath): Reads, parses, and returns the AST for a specific file (potentially caching ASTs in memory or on disk for performance).
            *   async function getDependencies(symbolIdentifier): Analyzes dependencies (based on parsed data) for a symbol and returns a list of identifiers of elements it depends on.
            Use JSDoc @typedef to clearly define the structure of objects returned by these API methods. Implement in-memory indexes (e.g., Map of symbol name -> list of HubEntry identifiers; Map of file path -> HubEntry list) to optimize lookup performance for a large number of entries.
    5.2. Routing Service (core/RoS.js)
        5.2.1. Query Analysis (function anlzQ(prompt, config, hubService)): Implement the function that analyzes the user's prompt to extract key information for routing.
            *   Tokenize the prompt string using a lightweight tokenizer.
            *   Identify keywords/phrases related to specific intents (e.g., "add function", "create class", "fix bug", "error in", "explain", "how to", "write tests", "document", "refactor"). Use regex or simple string matching patterns defined in config.
            *   Classify the primary intent based on identified keywords/phrases. Map to a predefined set of intent strings (e.g., 'generate_function', 'refactor_code', 'debug_error', 'explain_concept', 'answer_question', 'auto_code', 'test_generation', 'documentation_generation', 'other'). Default to 'answer_question' or 'other'.
            *   Detect potential code blocks within the prompt (e.g., using `` markers). Set a hasCode flag.
            *   Attempt to recognize potential code entities (function names, variable names, class names, file paths) mentioned explicitly in the prompt. Use hubService.findSymbol(term) to check if prompt terms match known symbols in the project Hub.
            *   Estimate complexity ('low', 'med', 'high') based on factors like prompt length, presence of code, number of entities identified, specificity of intent (e.g., "generate function X" might be 'med', "fix bug in X using Y and Z" might be 'high', "what is a closure" might be 'low').
            *   Return a structured qryInfo object: { complexity: string, intent: string, entities: string[], hasCode: boolean, promptLength: number, estimatedTokenCount: number }.
        5.2.2. Routing Decision Logic (async function routeQuery(prompt, options, { crid, services, history, userPreferences })): Implement the main routing orchestration logic. This function is called by API/CLI handlers.
            *   Call anlzQ(prompt, config, services.hubSvc) to get qryInfo.
            *   Build contextInfo (e.g., estimate potential context size based on qryInfo.entities).
            *   Get availableModels: Get the list of models marked as available: true from services.apiSvc.getAvailableModels() (which should read status from cfg.modelCapabilities updated by availability checks/circuit breakers).
            *   Select IRoutingStrategy: Choose a strategy (e.g., CostOptimizedStrategy, QualityOptimizedStrategy, BalancedStrategy) based on user preferences (options.strategy, userPreferences) or fallback to the default defined in config.defaultRoutingStrategy. Implement these strategies as separate modules/classes adhering to the IRoutingStrategy interface.
            *   Call the selected strategy's decide method: const routingDecision = selectedStrategy.decide(qryInfo, contextInfo, availableModels, config.routingRules, config.modelCapabilities);. The decide method applies routing rules (if any) and returns the RoutingDecision: { modelList: string[], contextStrategy: string, params: object }.
            *   Log the decision details (DEBUG level) with crid using services.logger.
            *   Return the RoutingDecision.
        5.2.3. Rules Engine (function matchRule(qryInfo, contextInfo, modelCaps, config)): Implement the core logic for evaluating routing rules. This function is used by routing strategies (or could be the primary routing mechanism itself).
            *   Load and validate config.routingRules (array of rule objects) using Zod schema.
            *   Iterate through the rules in the order they are defined.
            *   For each rule = { condition: object, action: object }:
                *   Evaluate the rule.condition against the provided qryInfo, contextInfo, modelCaps, and full config if needed.
                *   Support flexible conditions using nested objects and arrays to represent logical AND/OR, comparisons (equal, not equal, greater than, less than), array checks (includes, excludes), regex matching. Examples: { intent: { includes: ['generate_code', 'refactor_code'] } }, { complexity: { eq: 'high' }, ctxSizeKB: { gt: 100 } }, { modelCaps: { 'gpt-4': { available: true, quality: { gt: 4 } } } }.
                *   If rule.condition evaluates to true, return the rule.action: { preferredModels: string[], contextStrategy: string, params: object }.
            *   If no rules match, evaluate the condition of the final default fallback rule ({ condition: {_default: true}, action: {...} }).
            *   Return the action of the default rule.
        5.2.4. Fallback & Retry Orchestration (within calling flow, coordinated by RoS): The application flow that calls routeQuery (in main.js or API/CLI handlers) receives the modelList in the RoutingDecision. This flow is responsible for iterating through the modelList. For each model in the list, it calls ApiS.generate or ApiS.stream. If a call fails with a *retryable* error (as defined by ApiS custom error types), the flow catches it, logs the failure (logger.warn with crid), and attempts the *next* model in the modelList. If the list is exhausted or a *non-retryable* error occurs, the overall query fails. Implement a configurable maximum number of fallback attempts for a single query. Log fallback attempts and reasons with crid.
        5.2.5. Detailed Decision Logging: [CRITICAL FOR TUNING & DEBUGGING] Ensure comprehensive logging (DEBUG level) is generated for every routing decision. This includes: the inputs (qryInfo, contextInfo, availableModels), the selected strategy, the evaluation process of the rules engine (e.g., log which rules were evaluated, which conditions passed/failed), the final RoutingDecision (modelList, contextStrategy, params), and the associated crid.
        5.2.6. Model Capability & Runtime Availability: config.modelCapabilities defines static model properties (max tokens, cost, quality, supports streaming, base availability). ApiS should provide a mechanism (e.g., apiService.getAvailableModels()) that reflects *runtime* availability, potentially marking models as temporarily unavailable (e.g., based on circuit breaker state, specific API health check failures). RoS should use this runtime availability list when deciding. Implement logic to periodically check model health or rely on circuit breaker state changes to update this runtime availability status.
        5.2.7. Strategy Pattern Implementation: Define clear JSDoc @typedef interfaces for IRoutingStrategy and IContextStrategy (IContextStrategy definition from 5.4.1). Implement different strategies (e.g., CostOptimizedStrategy, QualityOptimizedStrategy) as separate modules/classes adhering to IRoutingStrategy. The decide method of these strategies will internally use the matchRule` function or implement their own logic based on the inputs.

    5.3. AI Models API Service (infrastructure/ai/ApiS.js)
        5.3.1. Unified Client Interface (IAiClient) and Implementations: Define a common interface (/** @typedef {{generate: (prompt:string, context:string, params:object, crid:string, signal?: AbortSignal) => Promise<string>, stream: (prompt:string, context:string, params:object, crid:string, signal?: AbortSignal) => AsyncIterable<string>}} IAiClient */). Implement concrete client classes (GptClient.js, ClaudeClient.js, OllamaClient.js, GrokClient.js, etc.) in infrastructure/ai/clients/ for each supported AI model API. Each client implements IAiClient, takes config and logger dependencies, handles model-specific API details (URL, payload format, headers, auth), and mapping the unified parameters (prompt, context, params) to the specific API requirements. Use a factory pattern (function createAiClient(modelName, config, logger)) to instantiate the correct client based on the requested model name.
        5.3.2. Robust HTTP Client Usage (axios/node-fetch): Use a robust, promise-based HTTP client library (e.g., axios is recommended for its interceptors and features, but node-fetch is also viable). Configure the HTTP client with appropriate default settings: baseURL, standard headers (e.g., Content-Type: application/json), connection pooling, and request timeouts (configurable per model). Use interceptors (axios) or wrap client calls in functions to centralize:
            *   Logging request/response summaries (method, URL, status, duration, bytes - redact sensitive data).
            *   Injecting the crid into an HTTP header (e.g., X-Correlation-ID) for API-side correlation if the external API supports it.
            *   Attaching AbortSignals to requests to enable cancellation from higher levels (e.g., graceful shutdown, request timeout logic).
        5.3.3. Centralized Error Handling, Retries, Circuit Breaking: Implement a robust error handling and resilience layer around AI API calls:
            *   [CRITICAL ERROR TYPING] Map standard HTTP status codes (400, 401, 403, 404, 429, 500, 503), and API-specific error codes/messages (from response bodies) to a consistent set of custom JS Error subclasses (e.g., extend a base ApiError with subclasses like AuthError, RateLimitError, NotFoundError, ModelError [for general API/model issues], NetworkError, TimeoutError, ContextLimitError). Include relevant details (status code, error code, response body snippet, original error) in the error objects.
            *   [CRITICAL RESILIENCE] Implement a circuit breaker pattern (opossum library is well-suited for this) around the calls to each individual external AI API endpoint. This protects the system from repeatedly hitting a failing or slow API, preventing cascading failures. Configure breakers with appropriate thresholds (errorThresholdPercentage, volumeThreshold), timeout (timeout), and resetTimeout. Log breaker state changes (open, close, halfOpen, reject) to the monitoring system (TelS). The RoS should check breaker state when determining model availability.
            *   Implement retry logic only for specific, known retryable errors (e.g., network errors, 503 Service Unavailable, 429 Rate Limit with Retry-After header). Use exponential backoff with added jitter (small random delay) between retries. Limit the number of retry attempts per specific call (maxRetries). This retry logic should ideally happen within the API client implementation or a wrapper layer, transparent to the caller (unless the caller is orchestrating fallback across models).
        5.3.4. Token and Cost Tracking (trkCost): Implement the function trkCost(modelName, tokensIn, tokensOut, crid). This function is called after a successful API interaction. If the API response provides token counts (input/output), use those. Otherwise, use a reliable local tokenizer (gpt-tokenizer or similar, configured for the specific model's tokenization rules if known) to estimate tokensIn (prompt + context sent) and tokensOut (response received). Calculate the estimated cost using the model's cost per token (cfg.modelCapabilities[modelName].costPerToken). Send an event (telemetryService.recEvt('api.tokens', { crid, model: modelName, tokensIn, tokensOut, estimatedCost })) to the TelemetryService for aggregation. Log token usage per call (INFO level) with crid.
        5.3.5. Streaming Implementation (async function* _xdStrm(...)): Implement the logic for handling and yielding streaming responses for models that support the stream method. This function receives an async iterator from the specific IAiClient implementation. Use a for await...of loop to consume text chunks as they arrive. Use yield chunk to pass chunks up to the caller (API endpoint, CLI handler). Implement error handling within the stream iteration (e.g., handle network errors, API errors signalled within the stream, premature connection closing). Ensure underlying resources (HTTP connection) are properly closed upon stream completion, error, or cancellation using finally blocks or AbortSignal.

    5.4. Context Builder Service (core/CtxB.js)
        5.4.1. Context Strategy Implementation (IContextStrategy): Implement distinct context building strategies as separate modules/classes in core/contextStrategies/. Each must adhere to the /** @typedef {{build: (qry:string, qryInfo:object, history:object[], config:object, dependencies:object)=>Promise<string>}} IContextStrategy */ interface (adjusted from previous definition to take more inputs). Strategies will primarily use the HubS API to gather code context.
            *   FullHubStrategy: Assembles a context string from all relevant entries in the in-memory Hub snapshot. Primarily serves as a simple fallback or for very small projects/queries. Prone to exceeding token limits.
            *   AstBasedStrategy: [PRIMARY INTELLIGENT STRATEGY] Aims for high relevance by providing code structure. Takes qryInfo (especially entities). Uses hubService.findSymbol to locate entities. Uses hubService.getSymbolDetails to get AST node positions and types. Reads relevant source file content(s) (hubService.getFileContent or fs.promises). Traverses the file's AST using acorn-walk (or re-parses if AST not cached) starting from identified nodes. Gathers code snippets (fileContent.slice(startOffset, endOffset)) for definitions of the entities, directly related code elements (e.g., parent function/class, variables used), relevant imports/exports, and potentially elements called from the entity or calling into the entity within a configurable proximity or dependency depth. Requires careful logic to determine "relevance" and limit the scope to avoid excessive context. Formats snippets clearly in the final context string, adding comments like // File: path | Symbol: name. Aims to provide highly relevant code structure.
            *   RagStrategy: [RAG Integration] Requires embeddings generated and stored by HubS. Takes the raw prompt. Generates a query embedding using the configured embedding model/API (via apiService). Performs vector search (using a local vector search library or dedicated database, e.g., cosine similarity) against Hub embeddings. Retrieves the top-K most similar code/documentation snippets (ranked by similarity score). Formats the content of these retrieved snippets into the context string, adding source comments. Needs configuration for the embedding model endpoint, vector store path/URL, search parameter k. Consider hybrid approaches combining RAG results with AST context.
        5.4.2. Context Size Optimization (trimCtx): Implement a function trimCtx(contextString, maxTokens, strategyName, crid):
            1.  Use a reliable local tokenizer (gpt-tokenizer or similar, configured for common models) to accurately estimate the token count of the contextString.
            2.  If tokenCount > maxTokens:
                *   Apply a smart pruning strategy based on strategyName and potentially metadata embedded in the context string (e.g., source comments).
                *   Pruning order examples: Remove least relevant RAG results first -> remove code comments/JSDoc -> remove nodes furthest from core entities (AST strategy) -> truncate large code snippets -> remove less important history turns -> apply final string truncation if necessary.
            3.  Log details of any truncation or pruning (original size, final size, method used, number of items removed) with the crid (DEBUG level).
        5.4.3. Conversation History Management (addHist): Implement function addHist(contextString, history: { role: 'user'|'assistant', content: string }[], config): Takes an array of previous conversation turns. Formats history turns appropriately (e.g., "User: ...\n", "Assistant: ...\n"). Prepends the formatted history string to the code/RAG context. Enforces configurable limits on history length (number of turns, config.context.historyTurns) and total history token budget (estimated using tokenizer, config.context.historyTokenLimit). Prunes the oldest turns if the budget is exceeded before adding to the main context.
        5.4.4. Data Redaction Integration: [CRITICAL FOR SECURITY] Ensure the final assembled context string (after combining code context, RAG snippets, and history) is passed through the redact(finalContextString) utility function (4.2.3) using configured sensitive patterns just before the context is tokenized for size check and sent to the ApiS.

    5.5. Code Editor Service (core/EditS.js)
        5.5.1. AST Node Localization (async function findAstNode(projectRoot, filePath, criteria: { name?: string, line?: number, col?: number, nodeType?: string, codeSnippet?: string })): Implement the function to precisely locate code elements based on various criteria.
            *   Construct full path: const fullPath = path.join(projectRoot, filePath);.
            *   Read file content: const fileContent = await fs.promises.readFile(fullPath, 'utf8');.
            *   Parse code: acorn.parse(fileContent, { ecmaVersion: 'latest', locations: true, ranges: true, /* ... */ }). Handle parse errors (log warning).
            *   Traverse AST: Use acorn-walk to search the AST based on the provided criteria. Criteria can include searching by node type (e.g., 'FunctionDeclaration'), by name (identifier), by line/column range, or by matching a small codeSnippet.
            *   Return value: Return an array of matching nodes. Each node object should include { start: { line, col, offset }, end: { line, col, offset }, nodeType: string, code: fileContent.slice(start.offset, end.offset) }.
        5.5.2. Diff Generation (mkDiff): Implement the function to generate a standard unified diff string. Use a reliable library like diff or diff-match-patch.
        5.5.3. Apply Changes (async function applyChanges(filePath, proposedNewContent, opts: { confirm?: boolean, nodeRange?: { start, end }, crid: string, projectRoot: string, logger: object, telemetryService: object })): Implement the core logic for applying modifications safely:
            1.  [CRITICAL SAFETY] Construct full path and read original file content: const fullPath = path.join(opts.projectRoot, filePath); const originalContent = await fs.promises.readFile(fullPath, 'utf8');.
            2.  [CRITICAL SAFETY] Create timestamped backup: Generate a unique backup path (e.g., ${fullPath}.bak.${Date.now()}.js). await fs.promises.writeFile(backupPath, originalContent);. [CRITICAL] Throw a specific error (FileSystemError) if backup creation fails, and abort the entire apply process. Log backup event (logger.debug).
            3.  Determine the final code string after applying the change: If opts.nodeRange is provided (typically comes from AI response after being found via findAstNode), perform precise string splicing using nodeRange.start.offset and nodeRange.end.offset on the originalContent string, inserting proposedNewContent. If opts.nodeRange is not provided (e.g., for creating a new file, appending to end, or a simpler AI output), use proposedNewContent directly or append it.
            4.  Programmatic Formatting: Use the Prettier API (prettier.format(modifiedContent, { parser: 'babel', ...config.prettierOptions })) to automatically format the resulting code according to configured style guides. Wrap in try/catch and log formatting errors, using the unformatted code as fallback.
            5.  Generate Diff: const diff = mkDiff(originalContent, formattedContent);.
            6.  Handle Confirmation: If opts.confirm is true: Log the generated diff (logger.info with crid). In CLI mode, use inquirer to display the diff and ask for user confirmation ("Apply changes? [y/N/d(iff)]"). In API mode, return the diff and await a separate confirmation API call (this requires tracking pending edits server-side). Abort if not confirmed.
            7.  Write Changes: Write the formattedContent to the original file path (await fs.promises.writeFile(fullPath, formattedContent);). Handle potential file system write errors (try/catch, throw FileSystemError).
            8.  Store Backup Mapping: Store the mapping { filePath: filePath, backupPath: backupPath, timestamp: Date.now(), crid: opts.crid } persistently (e.g., in TelS SQLite DB) to enable the undo functionality even after application restarts.
            9.  Log success/failure with crid.
        5.5.4. Undo Functionality (async function undoLastChange(filePath, { crid, projectRoot, logger, telemetryService })): Implement the logic to revert the most recent change applied to a file.
            *   Retrieve the latest backup record for filePath from persistent storage (TelS DB).
            *   Log the undo attempt with crid, filePath, backupPath.
            *   If a backup exists: Copy the backup file over the original file path (fs.promises.copyFile(backupPath, path.join(projectRoot, filePath))).
            *   Remove the backup record from persistent storage and optionally delete the backup file itself (fs.promises.unlink(backupPath)).
            *   Handle errors robustly: Log an error if no backup is found for the file, or if file system errors occur during the copy/delete operations. Throw a user-friendly error if no backup found.
    5.6. Cache Service (infrastructure/cache/CchS.js)
        5.6.1. Multi-Level Cache Setup (L1 RAM + L2 Persistent SQLite): Initialize the L1 in-memory cache instance using node-cache. Configure it with maxKeys, stdTTL (default TTL for items added to L1 from L2, less than L2 TTL), checkperiod, useClones: false (to avoid cloning overhead). Initialize the L2 persistent cache by establishing a connection to the SQLite database file (config.cacheDbPath) using better-sqlite3. [CRITICAL PERFORMANCE/CONCURRENCY] Ensure Write-Ahead Logging (WAL) mode is enabled (db.pragma('journal_mode = WAL')) for better performance and concurrent read access, even with synchronous write calls internally (Node's better-sqlite3 is synchronous relative to JS event loop, but WAL manages concurrent OS-level access). Define and execute the SQLite schema creation script (cache.sql) on startup if the database is new: key TEXT PRIMARY KEY, value TEXT (response data), metadata TEXT (JSON string containing original TTL, context strategy, model name, etc.), expires_at INTEGER (Unix timestamp), created_at INTEGER (Unix timestamp). Create indices on key (implicit PK) and expires_at (CREATE INDEX idx_cache_expires ON cache(expires_at);) for fast lookups and expiry cleanup. Prepare reusable SQL statements (db.prepare()) for INSERT OR REPLACE, SELECT, DELETE.
        5.6.2. Cache API Implementation (async function get/set/invalidate/clrExp): Implement the asynchronous public API methods for the cache service.
            *   get(key, crid): Check L1: cache.get(key). If hit, log L1 hit to TelS, return value. If L1 miss: Log L1 miss to TelS. Check L2: Execute prepared SELECT value, metadata FROM cache WHERE key = ? AND expires_at > unixepoch() statement. If L2 hit: Log L2 hit to TelS. Parse metadata. Calculate remaining TTL (remainingTtl = expires_at - current_timestamp). Set the item in L1 (cache.set(key, value, remainingTtl) or L1's stdTTL). Return value. If L2 miss (or expired in L2): Log L2 miss to TelS. Return undefined/null.
            *   set(key, val, ttlSeconds, metadata, crid): Set L1: cache.set(key, val, ttlSeconds). Log L1 set to TelS. Prepare L2 data: expires_at = Math.floor(Date.now() / 1000) + ttlSeconds. Convert metadata to JSON string. [ASYNC L2 WRITE] To avoid blocking the caller or the event loop with synchronous SQLite writes, queue the L2 write operation. Use setImmediate to schedule the L2 write in the next event loop tick, or implement a dedicated asynchronous queue within the Cache Service that processes L2 write operations sequentially in the background. Execute the prepared INSERT OR REPLACE INTO cache (...) VALUES (?, ?, ?, ?, unixepoch()) statement. Log L2 set to TelS. Handle potential L2 write errors asynchronously (log error, but don't throw to caller).
            *   invalidate(keyPattern, crid): Delete matching keys from L1 (cache.del(keyPattern)). Execute L2 DELETE FROM cache WHERE key LIKE ? statement (or potentially more complex pattern matching if needed). Log invalidation event to TelS.
            *   clrExp(crid): Implement the periodic cleanup task. Use setInterval to call this function (e.g., every hour). Execute L2 DELETE FROM cache WHERE expires_at < unixepoch(). Log the number of expired entries removed to TelS.
            *   getStats(): Return current L1 (from node-cache stats) and L2 (query L2 table count) hit/miss/set counts.
        5.6.3. Cache Key Generation (genKey): [CRITICAL FOR CACHE HIT RATE] Implement a function genKey(prompt, contextSummary, modelName, params) that generates a deterministic, consistent, and unique key for each query/context/model/param combination. Use crypto.createHash('sha256').update(inputString).digest('hex'). The inputString must be generated consistently:
            *   Include the full prompt string.
            *   Include a representation of the context. Using the raw context string is simple but might be large. A better approach is a deterministic hash of a structured context summary object. This summary object should contain key identifiers for the context used (e.g., { strategy: 'AST', entities: sortedEntityNames, contextFileHashes: sortedFileHashes, historyHash }). Sort array elements (e.g., sortedEntityNames) and object keys before stringifying to ensure consistency: JSON.stringify(sortObjectKeys(contextSummaryObj)).
            *   Include modelName.
            *   Include relevant params used for the AI call (e.g., temperature, top_p). Deterministically stringify the params object (JSON.stringify(sortObjectKeys(params))).
        5.6.4. Cache Statistics Tracking: Internally track L1/L2 hit, miss, set counts. Provide getStats() method. Send relevant events (cache.hit.l1, cache.miss.l2, etc.) with crid and the cache key (or key hash) to TelS for aggregation and analysis (7.1.2, 8.2.3, 9.1).

    5.7. Telemetry Service (features/telemetry/TelS.js)
        5.7.1. Metrics Collection (recEvt): Implement a function recEvt(eventName, metadata) that acts as the central point for collecting telemetry events from all parts of the application. Use a predefined set of eventName constants (e.g., 'query.start', 'query.end', 'api.call.start', 'api.call.success', 'api.call.error', 'cache.hit.l1', 'hub.update.complete', 'editor.apply.success', 'feedback.received'). The metadata object should contain relevant details for the event, always including the associated crid. Examples: { crid, prompt, options } for query.start; { crid, model, duration, status, errorType? } for api.call.end; { crid, file, backupPath } for editor.apply.success. Collect detailed metrics like query counts (total, per model, per intent, per strategy), token usage (input/output per model), estimated cost (per model, total), latency (total query, API call, context build, routing decision duration histograms), cache hit/miss ratios, error counts (per model, per service, error type), Hub update duration/frequency, code edit/undo success/fail counts, user feedback ratings. Use Prometheus metric types (counters, gauges, histograms) within TelS to store aggregated in-memory data ready for exposition via the /metrics endpoint.
        5.7.2. Data Aggregation and Persistence: Implement logic within TelS to aggregate incoming events. For Prometheus metrics, this is done via prom-client. For detailed analysis and historical reporting, raw event data or aggregated summaries need to be persisted. Use the TelS SQLite DB (config.telemetryDbPath) for this. Store raw events in an events table (timestamp, crid, eventName, metadata JSON). Implement background aggregation tasks (e.g., scheduled via setInterval or a worker thread) to process recent raw events and store aggregated summaries (e.g., hourly, daily counts and sums per model/intent/strategy) in separate summary tables. Clean up old raw events periodically.
        5.7.3. Savings Calculation (async function calcSav()): Implement a function that queries the aggregated telemetry data from the TelS DB. Calculate the actual total cost incurred (sum of estimatedCost from api.tokens events). Calculate the hypothetical baseline cost by estimating the cost if all queries had been routed to the most expensive model (cfg.modelCapabilities): sum of (query.start events count * avg_tokens_per_query_for_that_model * cost_of_most_expensive_model). Calculate savings percentage and monetary value ((baseline - actual) / baseline * 100%).
        5.7.4. Telemetry Data API: Provide asynchronous methods for accessing telemetry data, primarily for the API (/v1/stats) and CLI (stats command):
            *   async function getStats(period): Queries aggregated data (e.g., from summary tables) for a specified time period (day, week, month) and returns a summary object (total queries, total cost, estimated savings, token usage per model, error summary, cache stats).
            *   async function getQueryLog(period, filter?): Queries raw event data (events table) for a period, potentially filtered by crid, model, intent, status. Returns a list of events for detailed tracing/debugging.
            *   async function getFeedback(period, filter?): Queries user feedback data from the feedback table.
        5.7.5. User Feedback Integration: Implement a function recFeedback(crid, rating, comment?) that stores user feedback data received from the API (/v1/feedback) or CLI (ai feedback). Store this data persistently (e.g., in a feedback table in the TelS DB) linked by the crid of the query the feedback refers to. This data is crucial for the adaptive routing analysis (9.2). Also implement storeBackupMapping(filePath, backupPath, crid) here to persist backup metadata.

-==PHASE 4==-
Phase 4: Testing (JS Comprehensive - Ensuring Quality & Reliability) (Est: 8-16 hours; Realistic: Weeks - Integrated Throughout Development Post-PoC)
    *   [Goal - Post-PoC]: Implement a rigorous, multi-layered automated testing strategy integrated throughout the development process to ensure the correctness, robustness, performance, and reliability of the full "Enhanced" system beyond the minimal PoC. Testing is crucial for managing complexity and confidence in deployments.
    7.1. Testing Strategy (Layered and Continuous):
        7.1.1. Unit Tests (vitest): [FOUNDATION OF CORRECTNESS] Implement a comprehensive suite of unit tests using vitest (or Jest if preferred). Aim for high code coverage targets (e.g., >80-90% statement, branch, function) for all core logic modules, utility functions, and isolated components. Use vitest with its built-in mocking capabilities (vi.fn(), vi.mock(), vi.spyOn()) to isolate units under test from their dependencies. Focus on testing: individual function logic with various inputs and edge cases, specific methods of classes/services in isolation, error handling paths within individual units, boundary conditions for logic, validation logic for Zod schemas and custom validators, correctness of algorithms (routing rule evaluation, cache key generation, context pruning). Utilize snapshot testing (toMatchSnapshot) judiciously for verifying the structure of complex but relatively stable outputs (e.g., generated context structure based on specific inputs, AST node data, complex configuration objects) but rely primarily on explicit assertions (expect(...)) for verifying logic. Maintain unit tests alongside the code they test, ideally following a Test-Driven Development (TDD) approach where practical (write tests before code).
        7.1.2. Integration Tests (vitest): [VERIFY SERVICE INTERACTIONS] Develop integration tests using vitest to verify the correct interaction, data flow, and collaboration between collaborating services and modules. Focus on testing the contracts and communication paths between modules at their integration points. Examples:
            *   RoS correctly calls CtxB and ApiS (mocked) based on routing decisions.
            *   CchS L1/L2 cache logic works correctly (get checks L1 then L2, set updates both, expiry works).
            *   EditS successfully interacts with HubS for AST data and performs file operations (backup, write, restore) correctly (potentially using mock-fs for controlled FS interaction or dedicated test directories).
            *   TelS accurately receives and records events triggered by actions in other services.
            *   Error propagation between services is handled correctly (e.g., ApiS error handled by RoS fallback logic).
            *   Integrate feedback loop testing: simulate user feedback submission via the TelS API and verify data storage; simulate the analysis process (manually or via test utility functions) and verify how it could inform Routing Service configuration tuning.
            *   Use test-specific configurations and lightweight dependencies (e.g., in-memory SQLite via :memory: connection string for isolation and speed, or temporary directories for Hub/backup files). Mock only the external boundaries of the system, primarily actual HTTP calls to external AI APIs (by mocking the ApiS client implementations) and external network dependencies. Test the integration of the feedback loop (simulate feedback -> verify TelS -> simulate tuning -> verify RoS change). Actively test for potential race conditions or concurrency issues if worker threads or asynchronous operations are heavily utilized in service interactions.
        7.1.3. End-to-End (E2E) Tests (vitest + supertest + execa): [VALIDATE USER WORKFLOWS] Develop end-to-end tests to validate complete user workflows from the external interfaces (CLI commands and API endpoints) through the entire application stack. These tests should run against a live instance of the application.
            *   Setup: For API tests, start the application as a separate process (or as a fixture if using vitest environment) and use supertest to send HTTP requests to its API server. For CLI tests, use execa to execute CLI commands in subprocesses. Use pre-defined test fixtures (sample code project directories, configuration files, potentially mock external services) to create consistent and repeatable test environments.
            *   [CRITICAL VALIDATION] Mock External AI APIs: [CRITICAL] Within E2E tests, it is essential to mock the external AI APIs at the network layer (e.g., using msw or a dedicated mock server) or at the ApiS client implementation layer. This allows for controlled test scenarios (e.g., simulate specific AI responses, simulate API errors, simulate rate limits) without incurring actual API costs or relying on external service availability and variability.
            *   Assertions: Verify API response status codes, headers, and bodies (correct JSON structure, streamed SSE content correctness). Verify CLI command output (stdout, stderr), exit codes. Assert on application logs (checked in a separate log file or captured via standard output redirection in the test runner) for specific messages, log levels (especially warnings and errors), and the presence of the correct crid for correlation. Assert on side effects: verify that files are created, modified, or deleted on the file system (within the controlled test fixture directories), check the state of test databases (cache entries created/deleted, telemetry events recorded, feedback stored), and verify the application's process status (running, exited, exit code).
            *   Scenarios: Cover key user workflows and critical paths: Successful ai ask command (streaming and non-streaming). Successful POST /v1/query request. ai edit command (with and without --confirm flag) to test the code editing workflow, verifying that code changes are applied correctly, backup files are created as expected, and the --diff and --confirm interactive flow functions as designed. Test the ai undo command to ensure reliable restoration from backups. ai stats and /v1/stats data accuracy. ai health and /health endpoint correctness based on mocked dependency states. Test API input validation by sending malformed requests to API endpoints and verifying appropriate 400 Bad Request responses with detailed Zod error messages. Test error handling and resilience by mocking API errors (transient and permanent) and verifying graceful fallback or error reporting. Implement basic concurrency tests for API endpoints using supertest capabilities or external tools like autocannon.
        7.1.4. Tooling and Test Environment Configuration: Configure the vitest.config.js file to define different test environments or setups for unit, integration, and E2E tests (e.g., using different global setup files). Use @vitest/coverage-v8 for coverage reporting; integrate coverage checks into the CI pipeline and enforce a minimum threshold (e.g., 80%+) for critical directories (src/core/, src/features/, src/infrastructure/) to ensure critical code is tested. Use sqlite3 CLI or Node.js library for inspecting test databases (:memory: or temp files) during test debugging. Use gpt-tokenizer within tests to verify context size calculations and token usage estimations. Ensure the test environment (NODE_ENV=test) is correctly configured to use test-specific database paths, mock services, etc.
    7.2. Unit Test Examples: Testing HubS parser logic with various complex JS/TS syntax examples, including malformed code snippets and different JSDoc formats. Testing RoS rule matching logic with various combinations of qryInfo, contextInfo, modelCaps. Testing CchS cache key generation consistency across runs for identical inputs and different serialization orders. Testing Zod schema validation for edge cases and invalid data in configuration and API payloads. Testing redact function against various known sensitive patterns and edge cases. Testing CtxB context trimming logic for correct token count estimation and pruning order based on strategy. Testing EditS AST node location finding by name, line/col, and code snippet for nested and complex code structures. Testing specific utility functions.
    7.3. Integration Test Examples: Simulating a complete query processing flow including RoS strategy selection, CtxB context building (mocking HubS data), CchS lookup (mocking L1/L2 states), and ApiS call (mocking API response). Testing the multi-level cache interactions by performing a sequence of get/set/expire/invalidate operations across L1 and L2, verifying state consistency and hit/miss counts. Testing the Code Editor Service flow for applying changes, ensuring backup creation, correct text manipulation (using sample file content), diff generation, and subsequent undo functionality. Testing the integration of the Telemetry Service by triggering events from other services (e.g., RoS decisions, ApiS calls, CchS hits/misses) and verifying that TelS accurately records and aggregates these events. Testing graceful shutdown sequence verification, ensuring DBs are closed and resources released. Testing ApiS retry logic by mocking specific transient API errors.
    7.4. E2E Test Examples: Running ai ask "explain function X in file Y" --stream (pointing to a real file in fixtures) and verifying the streamed code output. Sending a POST /v1/query request with specific options (e.g., requesting a certain model or context strategy) and verifying the JSON response or SSE stream. Running ai edit src/utils.js --name sortFunc --confirm "refactor using Array.sort" and interactively confirming the diff via simulated input, then verifying the file modification and backup. Running ai undo src/utils.js and verifying the file reverts. Running ai stats --period week --json and validating the JSON output structure and data against expected telemetry. Testing GET /health response based on mocked dependency states (DB down, API down). Sending invalid JSON to API endpoints and verifying 400 responses with Zod error details.
    7.5. Performance & Load Testing: Implement scripts using performance.now() for benchmarking critical internal operations (Hub parsing/indexing time for projects of different sizes, context building time per strategy, routing decision latency). Use tools like autocannon or k6 to script realistic load test scenarios against the API (especially /v1/query endpoint with mocked AI responses) to measure application throughput (RPS) and latency (P50, P95, P99) under load, identifying performance bottlenecks within the application logic itself. Run these tests periodically in CI or a dedicated performance testing environment.
    7.6. Structured Feedback Loop Integration & Testing: Ensure the feedback mechanism (ai feedback <crid>... / /v1/feedback) is testable. Write integration tests that simulate feedback submission and verify data storage in TelS. Write E2E tests for the feedback commands/endpoints. Implement a process within the development cycle to regularly review collected feedback (even from internal testing) and factor it into backlog prioritization and routing rule tuning (9.2).

-==PHASE 5==-
Phase 5: Deployment & Monitoring (JS Enhanced - Production Readiness) (Est: 2-4 hrs + Continuous; Realistic: Days+)
    *   [Goal - Production]: Prepare the application for deployment to production environments, focusing on configuration, observability, resilience, and automated deployment processes.

    8.1. Production Configuration & Preparation:
        8.1.1. Environment Configuration: Ensure the application strictly adheres to the twelve-factor app methodology regarding configuration. All configuration, especially secrets (API keys, database credentials) and environment-specific settings (ports, hostnames, log levels), MUST be supplied via environment variables (process.env). Set NODE_ENV=production on the production environment. Do NOT commit .env files containing secrets to version control. Document required environment variables in a .env.example file (committed, no secrets).
        8.1.2. Build Optimization (Optional but Recommended): If using a build step (npm run build with esbuild, swc, or tsc), configure it for production optimization (minification, dead code elimination, tree-shaking if applicable) to reduce bundle size, improve loading times, and potentially enhance runtime performance. Ensure source maps (.map files) are generated for production builds to aid debugging errors originating in minified code, but configure the web server/CDN not to expose these map files publicly.
        8.1.3. Process Management (PM2): Configure pm2 (or a standard service manager like systemd, depending on the deployment environment) using an ecosystem.config.js file to manage the application's process lifecycle in production. Key pm2 configuration settings should include:
            *   name: Set a unique and descriptive name for the PM2 application process (e.g., ai-routing-system-prod).
            *   script: Point to the application's main entry point (dist/main.js if a build step is used, or src/main.js if running directly from source).
            *   instances: 'max': Utilize PM2's cluster mode to automatically run multiple instances of the application (typically one per CPU core) for increased concurrency, load balancing, and improved performance under load.
            *   exec_mode: 'cluster': Explicitly set the execution mode to 'cluster' to leverage Node.js cluster module's load balancing across the 'max' instances.
            *   watch: false: Disable PM2's file watching in production environments to avoid unnecessary CPU overhead and potential issues with file locking during updates.
            *   restart_delay: 1000: Configure a delay (e.g., 1000ms) before attempting to restart a crashed process, preventing rapid restart loops.
            *   max_restarts: Optionally set a limit on the number of restarts within a time window (e.g., max_restarts: 10 within restart_interval: 60000) to prevent persistent failure loops.
            *   log_file, error_file, out_file: Define paths for PM2 to capture application logs from stdout and stderr into managed log files, facilitating log rotation and centralized collection.
            *   env_production: Define and inject production-specific environment variables into the PM2 managed process (e.g., NODE_ENV: 'production', LOG_LEVEL: 'info', API_PORT: 80, DB_PATH: '/var/data/ai-router/cache.db', TELEMETRY_DB_PATH: '/var/data/ai-router/telemetry.db', and the real API keys loaded from the system environment or a secrets management system).

    8.2. Advanced Observability Setup (Integrated Monitoring Stack): Implement a comprehensive observability stack for production monitoring, performance analysis, and alerting.
        8.2.1. Metrics Exposition (/metrics Endpoint): Implement a dedicated HTTP endpoint (GET /metrics) in the API server using the prom-client library. This endpoint will expose application metrics in a format consumable by Prometheus. Ensure it includes:
            *   Custom application metrics registered by TelemetryService (counters for total queries, errors, cache hits/misses; histograms for query latency, API call duration, context build time; gauges for current in-memory counts, resource usage). Use meaningful metric names (e.g., prefixed with ai_router_) and labels (e.g., {model="gpt-4", intent="refactor", status="success"}).
            *   Default Node.js process metrics (CPU usage percentage, memory consumption breakdown, event loop lag, garbage collection activity) collected automatically by prom-client.collectDefaultMetrics().
            Configure a Prometheus server to scrape this endpoint periodically.
        8.2.2. Centralized Structured Logging Integration: Configure the Pino logger (infrastructure/logger.js) to output logs in structured JSON format to stdout. This is the standard and most efficient method for modern deployments. Deploy a log shipping agent (e.g., Filebeat, Promtail, Fluentd) alongside the application processes on the production server (e.g., as a sidecar container or a host agent) to collect these JSON logs from stdout/stderr and forward them to a centralized log management system (e.g., Elasticsearch + Kibana (ELK stack) or Loki + Promtail + Grafana). Ensure logs include the crid for correlation, timestamps, log levels, and structured metadata.
        8.2.3. Visualization Dashboards (Grafana): Build comprehensive Grafana dashboards using Prometheus as the primary data source for metrics and ELK/Loki as the data source for logs. Key dashboards/panels to create include:
            *   Overall System Health: Request throughput, error rate trends, overall latency (average, P95), process resource utilization (CPU, Memory), Event Loop Lag.
            *   Routing & Cost Analysis: Query volume per model, per intent, per routing strategy. Estimated cost per model, total cost over time. Cost savings analysis.
            *   Performance Breakdown: Latency percentiles (P50, P95, P99) for API calls, context building, routing decisions, grouped by model, strategy, or intent. Cache hit ratios (L1, L2) over time.
            *   Error Insights: Error counts per service, model, and error type. Ability to filter and analyze error logs based on metadata.
            *   User Feedback: Visualization of user feedback ratings over time, grouped by model or routing rule.
            *   Log Exploration: Panels allowing searching, filtering, and correlation of log messages based on crid, log level, service, model, etc.
        8.2.4. Alerting System (Alertmanager): Configure alerting rules in Alertmanager (integrated with Prometheus, receiving alerts via a Pushgateway or direct integration) based on the exposed metrics. Define critical alerts that trigger notifications to the development/operations team (e.g., via email, Slack, PagerDuty). Example alert rules:
            *   High API error rates for specific models (e.g., rate(ai_router_api_call_error_total{status="fail"}[5m]) > 0.15 - if 15% of calls to GPT-4 fail over 5 minutes).
            *   Service availability issues (e.g., health check endpoint returning non-200 status, or PM2 detecting frequent restarts: up{job="ai-router"} == 0).
            *   Performance degradation (e.g., high average request latency: avg_over_time(ai_router_query_duration_seconds_p95[5m]) > 10).
            *   Significant drop in cache hit ratio (e.g., (sum by (instance) (rate(ai_router_cache_hit_l2_total[5m])) / sum by (instance) (rate(ai_router_cache_hit_l2_total[5m]) + rate(ai_router_cache_miss_l2_total[5m]))) < 0.3 - if L2 hit rate drops below 30%).
            *   Security-related events (e.g., spikes in AuthError counts).
            *   Resource exhaustion alerts (CPU/Memory usage above thresholds).
            *   Exceeding predefined API cost budget thresholds (e.g., increase(ai_router_estimated_cost_usd_total[1h]) > 10).
            Configure Alertmanager to send notifications (e.g., email, Slack, PagerDuty) to the development/operations team when alerts are triggered.

    8.3. Auto-Recovery and Resilience Strategies: Implement strategies to ensure the application is self-healing and resilient to transient and persistent failures in production.
        8.3.1. Process Management & Automatic Restarts: Rely on the chosen process manager (PM2 or systemd) to automatically restart the application process in case of crashes or unexpected exits. Configure appropriate restart policies (restart_delay, max_restarts) to prevent flapping. Use pm2 cluster mode for basic load balancing and handling instance failures.
        8.3.2. Health Checks (/health Endpoint): Implement a dedicated /health endpoint (GET) in the API server. This endpoint should perform quick checks of critical internal dependencies, including:
            *   Successful connection to the SQLite databases (Cache DB, Telemetry DB).
            *   Reachability and successful basic authentication/test calls to configured AI API endpoints (using ApiS._xdChkTst or a similar lightweight function in ApiS).
            *   Basic status of the Hub Service (e.g., is the watcher running, when was the last successful update).
            The /health endpoint should return an HTTP 200 OK status code if all critical checks pass within a short timeout (e.g., 1-2 seconds), and an HTTP 5xx error code (e.g., 503 Service Unavailable) with specific details about the failing dependency if any critical check fails. Integrate this /health endpoint with PM2's health check monitoring (pm2 start --health-check /health ...) and with external load balancer health probes to automatically remove unhealthy instances from circulation.
        8.3.3. Rate Limiting: Configure API rate limiting middleware (@fastify/rate-limit in Fastify) on public API endpoints that handle user requests (e.g., /v1/query, /v1/editor/apply, /v1/feedback). Define rate limit thresholds (e.g., maximum requests per minute/second per IP address or per authenticated API key, if applicable). Configure the middleware to return an HTTP 429 Too Many Requests status code when a client exceeds the defined limits. For PM2 cluster mode or multi-server deployments, use a shared store (e.g., Redis) for rate limiting state.
        8.3.4. Circuit Breaker Pattern: [CRITICAL FOR RELIABILITY] Implement the circuit breaker pattern using the opossum library around calls to each individual external AI API endpoint within the ApiS.js service. Configure circuit breakers with appropriate thresholds:
            *   timeout: Time after which a single call is considered failed.
            *   errorThresholdPercentage: Percentage of failed calls within a rolling window that opens the circuit.
            *   volumeThreshold: Minimum number of calls in the rolling window to trigger the threshold check.
            *   resetTimeout: Time the circuit stays open before allowing a trial request.
            *   Use apiBreaker.fire(...) instead of directly calling the underlying client. Configure logging listeners (.on('open', ...) , .on('close', ...), .on('reject', ...)) to log circuit breaker state changes to monitoring (TelS). Ensure RoS uses the breaker state (apiBreaker.opened) when determining model availability.

    8.4. Deployment & Continuous Delivery (CI/CD) Pipeline: Establish a fully automated CI/CD pipeline to streamline and standardize the build, test, and deployment process for new application versions.
        8.4.1. Automated Pipeline Stages: Define a CI/CD pipeline using a platform like GitHub Actions or GitLab CI. Configure stages to trigger automatically on pushes to main development branches and on creating version tags. Key stages: 1) Checkout Code, 2) Setup Node.js Environment, 3) Install Dependencies (npm ci), 4) Code Quality Checks (npm run lint), 5) Security Vulnerability Scan (npm audit --audit-level=critical), 6) Run Automated Tests (npm run test --coverage), 7) Build Application (if applicable, npm run build).
        8.4.2. Automated Deployment Stage: If all CI stages pass successfully, trigger the CD deployment stage. Implement an automated deployment process:
            *   Build Production Artifact: Create the final production build artifact (if not done in CI build stage).
            *   Secure Transfer: Securely transfer the build artifact to the production server(s) (e.g., using SSH, SCP, or deploying a container image).
            *   Remote Deployment Command: Execute a command on the production server(s) to deploy the new version. For PM2, this is typically pm2 startOrGracefulReload ecosystem.config.js --env production, which handles stopping old instances, starting new ones, and managing zero-downtime updates.
        8.4.3. Deployment Strategies & Rollback: Use pm2 startOrGracefulReload for basic rolling updates. For more critical applications or major version changes, consider implementing Blue/Green deployment strategies (running the old and new versions side-by-side and switching traffic) or Canary deployments (gradually rolling out the new version to a small percentage of users) orchestrated via CI/CD scripts and load balancer configuration. Have a documented and tested automated rollback procedure (e.g., a script to redeploy the previous stable version tag using Git and PM2) ready in case of critical issues are detected during post-deployment monitoring.
        8.4.4. Secrets Management in CI/CD: [CRITICAL SECURITY] Use the CI/CD platform's secure secrets management features (e.g., GitHub Secrets, GitLab CI Variables, integrate with HashiCorp Vault) to securely store and inject production API keys and other sensitive configuration data into the production environment only during the deployment process. Do not store secrets directly in the code repository, CI/CD configuration files within the repo, or unencrypted in the build artifact.
        8.4.5. Post-Deployment Monitoring: Immediately following each automated deployment, implement automated post-deployment checks (e.g., run a small suite of critical E2E tests against production, query the /health endpoint). Configure monitoring alerts to notify the team of any spikes in error rates, latency, or critical dependency failures shortly after a deployment.

-==PHASE 6==-
Phase 6: Maintenance & Future Development (JS Continuous Process - Ongoing Evolution)
    *   [Goal - Long-Term]: Ensure the system remains reliable, performant, cost-effective, and valuable over its lifecycle through continuous monitoring, adaptation, enhancement, and planned feature development. This phase represents the ongoing life of the project after the core features are built and deployed.
    9.1. Active Monitoring & Proactive Performance Optimization:
        9.1.1. Continuous Analysis: Establish a routine (daily/weekly) for reviewing Grafana dashboards, analyzing structured logs in ELK/Loki, and checking Alertmanager for active alerts. Look for trends (increasing latency, fluctuating costs, changing error rates, dropping cache hit ratios), anomalies, performance regressions, and critical alerts.
        9.1.2. Proactive Optimization: Use monitoring data (metrics, logs), profiling tools (perf_hooks, Node Clinic flame graphs/bubbleprof) and load test results to proactively identify and address performance bottlenecks within the application. Focus on areas with high latency or resource consumption. Optimize database queries (SQLite, add/tune indices), review hot code paths in the application logic, investigate memory usage patterns and potential leaks, tune garbage collection parameters in Node.js. Refine cache parameters (TTLs, sizes) based on observed hit rates and memory usage, balancing performance and data freshness requirements.
        9.1.3. Configuration Tuning & Resource Management: Continuously fine-tune application configuration based on production monitoring data and telemetry analysis. Adjust routing rules in rules.json, update cfg.modelCapabilities (e.g., quality ratings, cost per token assumptions if pricing changes, max tokens), tune cache settings (L1 max keys, L2 TTL), adjust logging levels temporarily for debugging specific issues. Optimize resource allocation for PM2 instances based on CPU/memory usage metrics.

    9.2. Adaptive Router Learning & Evolution (Data-Driven Tuning): [CRITICAL FOR LONG-TERM COST SAVINGS & QUALITY] Implement a continuous, data-driven process to refine and improve the routing and context building logic over time.
        9.2.1. Collect Response Quality Feedback: Continuously collect user feedback on the quality and relevance of AI-generated responses via the implemented ai feedback command and /v1/feedback API endpoint. Encourage users to provide ratings (e.g., 1-5 stars) and optional comments for each query, linking feedback to specific queries via the crid.
        9.2.2. Regular Analysis: Establish a regular process (e.g., weekly analysis sessions) to analyze the collected telemetry data from TelS.gStats and detailed query logs from TelS.getQueryLog, combined with user feedback data from TelS.getFeedback. Use scripts (e.g., Python with pandas, Node.js with data analysis libraries) or BI tools to answer key questions:
            *   Effectiveness of Rules: Which routing rules lead to the highest/lowest user satisfaction (ratings), lowest cost per query, highest latency, or highest error rates for specific intents/complexities/context sizes?
            *   Model Performance: Which models perform best (quality, latency, cost) for specific types of queries or context strategies? Do quality ratings align with advertised model capabilities or costs?
            *   Context Strategy Effectiveness: Which context strategies result in the best user ratings or performance for specific queries? Are there patterns in cases where a strategy fails or provides insufficient context?
            *   Cache Performance: Analyze cache hit ratios over time and identify if certain query types or context patterns have low hit rates, suggesting potential improvements in key generation or cache size/TTL settings.
        9.2.3. Semi-Automated Rule Tuning & Optimization: Based on the insights from the data analysis, develop scripts that can suggest specific modifications to the routing rules defined in rules.json or the modelCapabilities configuration. Examples: suggest adjusting the preferredModels list for a rule that consistently yields low ratings for its cost, suggest using a different contextStrategy for a rule that results in high context size/cost or low relevance, suggest adjusting qualityRating or costPerToken values based on observed performance. Implement a structured process for reviewing these script-generated suggestions (e.g., in code review) and manually applying the approved changes to the configuration files (rules.json, config/config.json).
        9.2.4. A/B Testing Framework (Experimentation): For more advanced data-driven optimization and risk management, implement a framework for A/B testing routing changes. Use feature flags or a deterministic user segmentation strategy (e.g., hashing the user's ID or crid) to route a small percentage of traffic (e.g., 5-10%) for specific query types through an alternative, experimental routing configuration or to a new model. Tag telemetry data for A/B test groups to enable comparative performance analysis. Use A/B testing to validate the impact of proposed routing rule changes or new model integrations before rolling them out to all users.

    9.3. Continuous Core System Enhancements:
        9.3.1. Prompt Engineering & Model Adaptation: Stay updated on the latest advancements and best practices in prompt engineering for various LLMs. Regularly review and refine the system prompt templates used for specific tasks (code generation, explanation, refactoring) and for different AI models to ensure they are optimized for response quality, clarity, and conciseness. Conduct experiments with prompt variations to improve the clarity, conciseness, and effectiveness of AI outputs. Adapt prompt templates as needed to leverage the unique capabilities or mitigate the known weaknesses of specific AI models, especially as models are updated or new models are integrated.
        9.3.2. Parser & Code Analysis (HubS): Continuously enhance the HubService parser and analysis logic. Ensure compatibility with new JavaScript and TypeScript language features and syntax additions. Improve the accuracy and robustness of dependency analysis, type inference (even for dynamically typed JavaScript), and code structure extraction (e.g., understanding code blocks beyond simple functions). Explore deeper static analysis techniques (control flow graphs, data flow analysis) if needed for more advanced context building or code understanding features required by future features.
        9.3.3. Context Strategies (CtxB): Research, prototype, and implement new context building strategies to improve relevance and effectiveness. Examples: More sophisticated RAG techniques (hybrid search combining vector similarity with keyword/metadata filtering, re-ranking retrieved documents), context compression or summarization models (using smaller LLMs to summarize large context blocks), integrating external knowledge sources (e.g., README files, project documentation, API references) dynamically based on query entities. Optimize existing strategies for performance (speed of context building) and relevance.
        9.3.4. Refactoring & Dependency Updates: Proactively manage technical debt. Regularly schedule time for code refactoring, focusing on complex or brittle parts of the codebase identified during development or via monitoring. Update project dependencies regularly (npm update) to leverage new features, performance improvements, and patch security vulnerabilities. Ensure that tests are updated and pass after dependency updates.
    9.4. New Feature Planning and Implementation:
        9.4.1. Product Roadmap & Backlog Management: Maintain a prioritized product backlog of all potential new features, improvements, bug fixes, and technical debt items in a dedicated issue tracker (e.g., GitHub Issues with labels/milestones/projects). Regularly groom the backlog, adding new ideas (e.g., from user suggestions, internal brainstorming), refining existing ones with clear requirements and acceptance criteria, and prioritizing items based on a combination of user value, strategic goals (cost savings, developer productivity), estimated development effort, and technical feasibility.
        9.4.2. Potential Future Feature Examples: Plan and implement new features based on the prioritized backlog. Examples:
            *   Deeper IDE Integration: Develop IDE extensions (e.g., for VS Code, IntelliJ IDEA) to provide seamless integration, such as inline code suggestions, context-aware refactoring actions directly within the editor, visual interfaces for interacting with the Hub and AI responses, and interactive code editing feedback. Consider implementing a Language Server Protocol (LSP) server to enable richer IDE integration features (code completion, go-to-definition, find references) powered by Hub data.
            *   Autonomous Agent Capabilities: Explore implementing simple autonomous agent capabilities where the AI assistant can break down a larger task into smaller steps, execute them sequentially (including code editing, running tests, etc.), and report progress, automating repetitive coding tasks.
            *   Support for More Languages: Extend the system to support additional programming languages beyond JavaScript/TypeScript (e.g., Python, Java, Go, Rust) by integrating parsers (e.g., Tree-sitter, language-specific libraries) and code analysis tools for these languages into the Hub Service and adapting context building and code editing logic accordingly.
            *   Code/Hub Visualizations: Develop visual tools (e.g., web-based dashboards or IDE extensions) to visualize the project's codebase structure, module dependencies (based on Hub data), code complexity metrics, and potentially even the flow of AI interactions or telemetry statistics, providing developers with a better understanding of their codebase and the AI assistant's behavior.
            *   Integration with Project Management Systems: Explore integration with project management systems (e.g., Jira, Asana, Trello) to allow users to link AI-assisted coding tasks to project tickets, update task status, and facilitate team collaboration around AI-generated code suggestions.
            *   Collaborative Features: Features supporting multiple developers in a team using the assistant on a shared codebase.
        9.4.3. Implementation Process for New Features: Follow an iterative, agile development process. For each new feature, include design, implementation, rigorous testing (unit, integration, E2E), and documentation updates. Use feature flags (fflip or similar libraries) to enable gradual rollout of significant new features or major refactoring efforts, allowing for testing in production with a subset of users before full release to manage risk.

-==IMPORTANT SECTION==-
Implementation Strategy (JS Revised - Full System Timeline - Iterative Sprints)
    *   [Goal]: Outline a realistic, iterative development process for the full "Enhanced" system described in this plan, acknowledging the initial 24-hour PoC limit and providing a structure for ongoing development.
    *   [CRITICAL NOTE]: The 24-hour PoC covers only a minimal subset of Sprint 0/1 tasks (specifically section 2.2) and is focused only on validating the absolute core request path (input -> minimal call -> output -> basic log). The following sprints represent the timeline and estimated effort for building the full system described in the "Enhanced" sections (Phases 1-6) and require significantly more than 24 hours total. Development beyond Sprint 3 is ongoing.

    11.1. Sprint 0 (Est 2-4 days): Phase 1 - Project Setup & Foundation:
        *   Tasks: Full JS project setup (environment, dependencies, tooling, linting, formatting, CI configuration, Git hooks), initial architecture design (directory structure, module approach, DI principles), implementing basic logger and configuration modules (with Zod validation for config), basic custom Error classes, defining core service interfaces (JSDoc @typedef). Define Zod schemas for config/hub/API payloads.
        *   PoC Target within Sprint 0 (24h): Focus exclusively on section 2.2. Implement the minimal setup (2.2.1) and minimal core logic/interface (2.2.2, 2.2.3). Achieve the PoC success criteria (2.3) within the 24-hour timeframe. This specific effort is prioritized over completing all of Sprint 0's broader tasks within the 24h.
    11.2. Sprint 1 (Est 4-7 days): Phase 2 (Core Services - Basic Functional Impl): Implement basic, functional versions of the core services post-PoC. This includes: Hub Service (Acorn-based read+parse single file from test fixture, no file watching), basic functional RoS (simple rules, 2 models from config), basic ApiS (client implementations for 1-2 models using config, basic generate, basic error mapping), basic CtxB (FullHub strategy - concatenating config string), basic CchS (L1 RAM Map only), basic EditS (line-based code replacement in test files, basic .bak backup), basic TelS (in-memory counters for queries, API calls). Integrate these basic services within src/main.js (in the non-PoC path) to create a basic functional application flow when run with --mode api or as CLI. Implement essential unit tests (7.1.1) for these basic components (parser, simple routing rule, API client stub, cache Map).
    11.3. Sprint 2 (Est 4-7 days): Phase 3 + subset of Phase 4 (API/CLI & Essential Testing): Build the functional external interfaces. Implement the Fastify API server (6.2) including /v1/query endpoint (JSON request/response, no streaming yet), and /v1/status endpoint. Implement the functional Commander-based CLI (6.3) including the ask command (text output), and stats command. Implement essential integration tests (7.1.2) to verify the core query processing flow using mocked AI API calls and basic cache interactions. Develop a basic End-to-End (E2E) test (7.1.3) to validate the main CLI ask command workflow. Complete basic CI/CD pipeline setup (4.7.3) to automate linting, testing, and basic build processes on main branch.
    11.4. Sprint 3 (Est 1-2 weeks): Phase 5 (Basic Deployment & Stabilization) + Phase 2 Enhancements (Hub & Cache): Focus on deploying the MVP application to a staging or development environment using PM2 for process management (8.1). Set up basic monitoring (Pino logs to file, integrate with PM2 logs, basic /v1/status endpoint, maybe basic Prometheus exposition for a few metrics - 8.2 subset). Begin gathering initial user feedback and performance data from internal testing. Implement key initial enhancements to core services: Enhance Hub Service with file watching capabilities (Chokidar basic, maybe worker thread - 5.1.3). Implement L2 persistent cache using SQLite for the Cache Service (5.6). Implement more robust error handling throughout the application (central error handler, custom error classes - 6.2.5). Enhance Code Editor Service to support interactive code editing with --confirm flag and diff previews (5.5). [IMPORTANT] Begin structured logging to LLM_THINKING.txt by the LLM development assistant after each code save operation from this sprint onwards (3.2).
    11.5. Sprint 4+ (Continuous Iterations - Ongoing Enhancement & Feature Development): This represents the ongoing life of the project. Iteratively work through the prioritized backlog of remaining "Enhanced" features and improvements defined in Phases 2 through 6. Follow an agile development process with short sprints (e.g., 1-2 weeks). Prioritize backlog items based on a combination of user feedback (collected in Sprint 3+), telemetry data analysis (cost savings, performance bottlenecks observed in Sprint 3+ monitoring), and strategic roadmap priorities. Focus sprints on implementing and rigorously testing advanced features such as: AST-based parsing and code analysis (5.1), AST-based and RAG-based context building (5.4), advanced routing strategies and rules engine (5.2), streaming support (5.3, 6.2, 6.3), comprehensive testing (increasing coverage - Phase 4 becomes an ongoing activity), full observability stack (Prometheus, Grafana, Loki - 8.2), and resilience patterns (circuit breaker, full health checks, rate limiting - 8.3), telemetry analysis tools (5.7), user feedback collection endpoints (5.7), and new features (Phase 6). Continuously refine prompt templates, optimize performance, address technical debt proactively, and plan for new features based on user needs and evolving AI model capabilities. Implement and refine the adaptive routing logic (Phase 6) based on collected telemetry and user feedback to continuously improve the system's cost-effectiveness and response quality over time. Testing, JSDoc documentation updates, and refactoring are integrated into every sprint, not treated as separate phases.

-==IMPORTANT SECTION==-
Summary & Expected Benefits (Full System - Long-Term Value)
    12.1. Significant Financial Savings: [Target] Achieve and maintain substantial reductions in LLM API costs (target 65-75%+ savings) through intelligent, dynamic routing to cost-effective models based on query needs and aggressive, multi-tiered caching (L1/L2), maximizing return on investment (ROI) for AI-powered coding assistance.
    12.2. Improved Performance & Enhanced User Experience: Deliver a responsive and fluid user experience characterized by minimal latency (both perceived and actual) through optimized asynchronous operations, efficient caching mechanisms minimizing repeated API calls, and real-time response streaming allowing for immediate user feedback. Provide a clear, intuitive, and visually appealing CLI and API interface that supports modern UI design principles.
    12.3. Higher Quality & More Relevant AI Assistance: Enhance the quality, accuracy, and relevance of AI-generated code and responses by intelligently selecting the optimal AI model for each query based on deep analysis of query characteristics, rich project context (AST, RAG), and user-defined preferences. Implement advanced context building strategies to provide models with the most pertinent information for generating accurate and contextually appropriate outputs.
    12.4. Robust & Scalable Architecture: Build a well-architected, modular, and scalable JavaScript codebase designed for long-term maintenance, extension, and adaptation. The adherence to ES Modules, DI, SRP, comprehensive testing (unit, integration, E2E), and CI/CD ensures code quality, reliability, and facilitates continuous integration and delivery of new features and improvements. The TypeScript-ready structure allows for potential future migration.
    1.2.5. Enhanced Security & Data Privacy: Incorporate robust security measures to protect sensitive data (API keys via Env Vars), mitigate potential security risks associated with sending code and user data to external APIs (data redaction, input sanitization), prevent common vulnerabilities (dependency scanning), and follow secure coding practices throughout the application lifecycle.
    12.6. Adaptive & Learning System: [IMPORTANT DIFFERENTIATOR] Develop a system capable of continuous learning and adaptation based on real-world usage. Implement a robust telemetry and user feedback loop to collect data on performance (cost, latency, token usage) and user quality ratings. Utilize this data to continuously analyze routing effectiveness, identify areas for optimization, and implement a process (semi-automated tuning, A/B testing framework) to evolve and improve routing rules and model configurations over time, ensuring the system becomes increasingly cost-effective and high-quality.
    12.7. Increased Developer Productivity & Efficiency: Evolve the AI coding assistant into a powerful, reliable, fast, and context-aware tool that significantly accelerates developer workflows, reduces manual coding effort, and empowers developers to be more productive and efficient in their coding tasks. The tool will become an invaluable asset for software development teams seeking to leverage the power of AI while optimizing costs and maintaining high code quality.
-==IMPORTANT SECTION==-
----

return readme.md