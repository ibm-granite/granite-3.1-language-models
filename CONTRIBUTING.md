# Contributing to Granite 3.0 Language Models
We want to make contributing to this project as straightfoward as possible. With the introduction of new features such as code generation, refactoring, code analysis, debugging, documentation, and learning, we have updated our guidelines to help you contribute effectively.

## :memo: Issues
We use GitHub issues to track public bugs and inconsistencies. Plese follow these instructions to create an issue:
1. Create a bug/inconsistency issue by using [this](https://github.com/ibm-granite/granite-3.1-language-models/blob/main/.github/01_bug_inconsistency_report.md) template.
2. Make sure to provide all the information that the template requires. Please provide clear instructions on how to reproduce the issue.
3. For issues related to model evaluation, please include detailed information about the evaluation process, benchmarks used, and the results obtained.

## :hammer: Pull Requests
At present, we only welcome pull requests to correct bugs and inconsistencies. Before submitting a pull request please make sure to:
1. Create an issue by following the instructions provided in the previous section, and make sure to link it to your pull request.
2. Fork the repository and create your branch from `main`.
3. Please make sure your code lints.
4. Ensure the test suite passes.
5. If you've changed code examples, please update their respective documentation.
6. If you've added code that requires a new test, please include this test in your pull request.

## :star: License
By contributing to Granite Code Models, you agree that your contributions will be
licensed under [Apache 2.0](./LICENSE).

## :bar_chart: Model Evaluation
We welcome contributions to the evaluation of our models. To ensure consistency and reliability in the evaluation process, please follow these guidelines:

1. **Evaluation Benchmarks**: Use established benchmarks and datasets relevant to the tasks the models are designed for, such as multilinguality, coding, reasoning, and tool usage.
2. **Evaluation Metrics**: Document the metrics used for evaluation, such as accuracy, F1 score, BLEU score, etc.
3. **Reproducibility**: Provide detailed instructions and scripts to reproduce the evaluation results.
4. **Reporting Results**: Clearly document the evaluation results, including any observations or insights gained during the evaluation process.
5. **Comparison with Baselines**: Where possible, compare the evaluation results with baseline models or previous versions of the Granite language models.

## Creating Issues
To create an issue, please use our [Bug/Inconsistency Report Template](.github/01_bug_inconsistency_report.md). Make sure to provide all the required information and clear instructions on how to reproduce the issue.

## Submitting Pull Requests
To submit a pull request, please follow these steps:
1. Create an issue by following the instructions provided in the "Creating Issues" section, and make sure to link it to your pull request.
2. Fork the repository and create your branch from `main`.
3. Ensure your code lints.
4. Ensure the test suite passes.
5. If you've changed code examples, update their respective documentation.
6. If you've added code that requires a new test, include this test in your pull request.

## Guidelines for Contributing to Code Generation and Refactoring
We welcome contributions to the code generation and refactoring features of our models. To ensure consistency and quality, please follow these guidelines:

1. **Code Generation**: When contributing to code generation features, ensure that the generated code is clean, well-documented, and follows best practices. Provide examples and test cases to demonstrate the functionality of the generated code.
2. **Refactoring**: When contributing to code refactoring features, ensure that the refactored code is more efficient, maintainable, and adheres to coding standards. Provide before-and-after code examples to illustrate the improvements made through refactoring.
3. **Documentation**: Update the documentation to reflect any changes or additions to the code generation and refactoring features. Include usage instructions, examples, and any relevant guidelines for users.

## Guidelines for Contributing to Code Analysis and Debugging
We welcome contributions to the code analysis and debugging features of our models. To ensure effectiveness and reliability, please follow these guidelines:

1. **Code Analysis**: When contributing to code analysis features, ensure that the analysis is accurate, comprehensive, and provides meaningful insights. Provide examples and test cases to demonstrate the effectiveness of the code analysis.
2. **Debugging**: When contributing to debugging features, ensure that the debugging process is efficient, user-friendly, and helps identify and resolve issues effectively. Provide examples and test cases to demonstrate the debugging capabilities.
3. **Documentation**: Update the documentation to reflect any changes or additions to the code analysis and debugging features. Include usage instructions, examples, and any relevant guidelines for users.

## Guidelines for Contributing to Documentation and Learning
We welcome contributions to the documentation and learning features of our models. To ensure clarity and usefulness, please follow these guidelines:

1. **Documentation**: When contributing to documentation features, ensure that the documentation is comprehensive, clear, and well-organized. Include function descriptions, usage examples, and API references. Provide examples and test cases to demonstrate the functionality of the documented features.
2. **Learning Resources**: When contributing to learning resources, ensure that the resources are interactive, engaging, and help developers learn new programming languages and frameworks. Provide examples and test cases to demonstrate the effectiveness of the learning resources.
3. **Real-Time Suggestions and Explanations**: When contributing to real-time code suggestions and explanations, ensure that the suggestions are accurate, relevant, and help developers write better code. Provide examples and test cases to demonstrate the effectiveness of the real-time suggestions and explanations.
